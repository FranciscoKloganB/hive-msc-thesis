%% fsip2pnupsg.tex
%% V0.1
%% 2019/11/27
%% by Francisco Barros

%%%%%%%%%%%%%%%%%
% BEGIN IMPORTS %
%%%%%%%%%%%%%%%%%
\documentclass[runningheads]{llncs}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{optidef}
\usepackage{subfigure}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{todonotes}

\newcommand{\SubItem}[1]{{\setlength\itemindent{15pt} \item[-] #1}}
%%%%%%%%%%%%%%%
% END IMPORTS %
%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT %
%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%
% BEGIN HEADER %
%%%%%%%%%%%%%%%%
\title {Hives: File Survivability in P2P using Probabilistic Swarm Guidance
\thanks{This work has the support of the project MYRG2016-00097-FST of the University of Macau; by the Portuguese Fundação para a Ciência e a Tecnologia (FCT) through Institute for Systems and Robotics (ISR), under Laboratory for Robotics and Engineering Systems (LARSyS) project UID/EEA/50009/2019.}
}
\titlerunning{Hives}
\author{Francisco Barros, Daniel Silvestre \and Carlos Silvestre}
\authorrunning{F. Barros et al.}
\institute{Instituto Superior Técnico - Taguspark\newline Av. Prof. Doutor Cavaco Silva, 2744-016 Porto Salvo, Portugal
\email{fbarros@isr.ist.utl.pt, dsilvestre@isr.ist.utl.pt, csilvestre@umac.mo}\newline
\url{www.tecnico.ulisboa.pt}
}
\maketitle
%%%%%%%%%%%%%%
% END HEADER %
%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
% BEGIN ABSTRACT %
%%%%%%%%%%%%%%%%%%
\begin{abstract}
Peer-to-Peer (P2P) systems have emerged as a potential choice to build large scale distributed storage systems, at a fraction of the cost of the alternative cloud approaches, which have a better quality of service guarantees. We survey general-purpose P2P protocols, distributed file systems, cloud-assisted P2P protocols, and erasure-code algorithms. We then draft a solution for the problem of creating a bio-inspired P2P overlay to be used on a cloud-assisted P2P storage and backup system using probabilistic swarm guidance. We anticipate that our solution will have high network overhead, in order to provide self-healing properties to the files stored in the system.

\begin{keywords}Agents-based Systems; Cooperative Control; Peer-to-Peer; Cloud Storage; Distributed control; File Availability; Swarm Guidance;\end{keywords}
\end{abstract}
%%%%%%%%%%%%%%%%
% END ABSTRACT %
%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN TABLE OF CONTENTS %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{tocdepth}{3}
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%
% END TABLE OF CONTENTS %
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% BEGIN INTRODUCTION %
%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Introduction}\label{sec:intro}

\subsection{Problem Description}
With the growth of the internet and the emergence of distributed systems, two large-scale computing paradigms have gained popularity due to their promise of virtually unlimited scalability. On the one hand, we have Peer-to-Peer (P2P) networking, which can be defined as a group of equally privileged peers, who contribute with a portion of their resources, to achieve common goals. These networks are popular among file sharing and streaming applications \cite{ssaroiu:msp2pfss}, due to their self-organized behavior, lack of centralization and, low-cost. On the other hand, Cloud platforms offer unmatched, on-demand, self-service, availability, and reliability, at a higher cost, and are currently trendy, with companies such as Amazon, Google, and Microsoft offering various ITaaS products to individuals and organizations alike. Cloud-based systems are centralized architectures, in which a large number of computers are clustered and managed by master entities, which possibly, become bottlenecks. Both paradigms can be used to deploy distributed file-storage or file-backup applications; despite the fact that P2P implementations are cheaper for both companies and their clients, these have a hard-time achieving 99.9\% availability seen Cloud-based implementations, furthermore, depending on the P2P application there may also exist the risk of permanent file loss, making them somewhat unappealing for clients who want to store or backup their files remotely.

\subsection{Our approach}
The focus of this thesis is to create a structured P2P overlay to be used on a distributed backup system, where clients upload their files to remote peers, who are selling their storage space, without persistence guarantees. Probabilistic Swarm Guidance (PSG) and Markov Chain theories \cite{markovchain_approach_to_pga} will be used, along with block-based file replication using erasure code algorithms and a reputation system, to hopefully create a network that is able to keep a file available at any given time, regardless of presence of attackers, network partitions and amount of churn, i.e., no matter how many peers leave/join an overlay.

\subsection
{Frequent approach}There has been much research on the topic of P2P overlays. There are two significant categories of overlays, Structured and Unstructured. In the former, the overlay has rigid rules regarding peer placement; they favor lookup speed but have higher maintenance overhead under churn, i.e., the process of peers leaving or joining the overlay. Some popular P2P Structured overlays include Chord \cite{chord}, Tapestry \cite{tapestry}, Kademlia \cite{kademlia}, and P-Grid \cite{pgrid}. In the latter, peer placement is random, which provides the overlay network with a higher degree of resilience and robustness in the advent of failures. Unstructured overlays disseminate information in a gossip-like fashion; thus, lookup operations are slower, resource management is less efficient, but overlay maintenance is often straightforward. Popular  Unstructured P2P overlays include FreeNet \cite{freenet}, Gnutella \cite{gnutella-rfc} and BitTorrent \cite{bittorrent}. Research on multi-layer overlays has not yielded satisfactory results, and implementation is difficult \cite{sotart}, but research on bio-inspired overlays did have promising results, some of these include Self-Chord \cite{selfchord}. Finally, there have also been some attempts of transparently integrating Cloud Systems into P2P networks, e.g., CLOUDCAST \cite{cloudcast}, and vice-versa, e.g., Spotify \cite{spotify} and Wuala \cite{wuala}. From all the mentioned research, perhaps the one that borrows most similarity with our proposal is CLOUDCAST; however, neither it nor any of the remaining literature, try to create a structured overlay composed of completely autonomous peers, who work together to keep the file alive without enforced persistence based on PSG.

\subsection{Proposal Goals}\label{subsec:intro}
\begin{itemize}
    \item Implement a Tracker server with light-weight responsibilities:
        \SubItem{Act as peer discovery service for clients;}
        \SubItem{Act as an indexer for clusters of peers (Hives);}
        \SubItem{Track peer reputations;}
    \item Implement an erasure-code algorithm to ensure file persists in the Hive;
    \item Implement a trust system where:
        \SubItem Each peer gives feedback to the Tracker about his neighbors;
        \SubItem Tracker synthesizes the feedback and presents it to the client;
    \item Calculate the ideal steady-state distribution of file blocks, for a Hive, when:
        \SubItem{No trust history on a new peer is available;}
        \SubItem{Peer availability, reliability and other trust factors are known;}
    \item Hives converge to ideal block distribution in bounded-times using PSG;
    \item Efficiently update the PSG's Markov Chain as it changes over time;
    \item Balance the in-degree and out-degree of each peer such that:
        \SubItem{Message exchange overhead is minimal;}
        \SubItem{Any changes to the probability of file loss are negligible;}
        \SubItem{Hive availability and reliability are not compromised;}
        \SubItem{Hive is resilient to massive node failures and resistant to churn;}
    \item Extra miles may include:
        \SubItem{Peers can reconstruct Hives without Tracker intervention;}
        \SubItem{Peers trust their neighbors independently of Tracker;}
        \SubItem{Hives can leverage dedicated Cloud-Storage if they become unreliable;}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%
% END INTRODUCTION %
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% BEGIN RELATED WORK %
%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Related Work}\label{sec:relatedwork}
This section as the following structure: In the first subsection, we will study two distributed file systems used in cloud architectures, in the following two subsections we look at four P2P overlays. We will then mention one cloud-assisted P2P protocol that we consider relevant for being designed to the same use-case that led us to make this thesis proposal. Then we will take a look at two well-known research regarding trust in P2P networks, and finally, we will study erasure code algorithms for file-block replication.
\subsection{Distributed File Systems}
\textbf{Google File System} (GFS) is designed to provide efficient and reliable access to data using clusters of commodity machines located in geographically spread data-centers. In particular, GFS aims to provide high data throughput, low latency, and reliability in the face of individual server failures. Because GFS runs over commodity hardware, the proposed model considers machine and network faults as being the norm rather than the exception \cite{gfs}.  This assumption is typical in modern distributed systems of any kind. GFS is the basis for an open-source project, which has seen continuous improvements over the years, called Hadoop Distributed File System (HDFS) \cite{hadoop}; we will not discuss it here because it holds many similarities to GFS with key differences being the approach to security management. One advantage of GFS over other architectures is that it leverages a distributed file system that tailors to the designers observation that, once written, files on google provided services, are only read, often sequentially, with random writes being practically non-existent, shifting optimization focus from latency to throughput, from random writes to appends when it comes to atomicity and no caching of data blocks \cite{gfs}. In the GFS architecture cluster consists of a single master and multiple chunk-servers. The Master maintains a mapping from files to chunks, the current location of chunks, and is responsible for failure detection over the chunk-servers as well as chunk-migration between them. Clients communicate, for reads and writes, with the Master for metadata operations and directly with the chunk-servers for data-bearing operations. Neither the client nor the chunk-servers cache file data because most of google's client applications stream through huge files. A read operation involves obtaining chunk handles and locations from the Master and then retrieving the chunks from one of the many available chunk-severs. For writes, the client splits the file into 64MB chunks and asks the Master to assign them an identifier, and then the client uploads his chunks to the chunk-servers indicated by the Master.

GFS has proven its qualities in supporting large-scale data processing. However, small files have shown \cite{gfs} to lead to overloads on chunk-servers holding them because chunks have a fixed large value size of 64MB. The reasoning for GFS to use such substantial value is to reduce network overload, reduce the number of accesses to the Master, and reduce memory used at the Master due to mappings. Concerning our proposal, Hives, since convergence to a desired steady-state using Markov Chains theory requires a large number of chunks to exist in a P2P Hive, we might have to do file-level replication rather than the desired block-level when client files are too small.\newline

\textbf{Ceph} is an open-source, storage system that, like GFS, runs over commodity hardware. Ceph aims to deliver reliable, autonomous, distributed object, block, and file storage that is self-healing, self-managing, and has no single point of failure. The base architectural component of Ceph is the RADOS object store, and all content a client reads and writes to the system are represented as objects, irrespective of the objects' original data type. RADOS is for all purposes a Ceph cluster, and its storage space originates from the disks of multiple machines. Ceph architecture is slightly more complicated than that of GFS, so we will summarize the components that are of interest to us. Inside a cluster, we find three types of machines, storage devices, abstracted with a layer called Object Software Daemon (OSD), Monitors, and Metadata Servers. A small count of Monitors maintain cluster membership and state using PAXOS \cite{paxos} to make consensual decisions. Metadata Servers only required sharing the distributed file system, are used for POSIX semantics, e.g., directory hierarchy and file metadata such as owner and last accessed timestamps. Monitors can detect if metadata servers are down and launch new ones as required. Finally, OSDs store clients' data, and there are anywhere between dozens to thousands of these in a Ceph cluster.

The innovation Ceph brings is that the OSDs within a cluster are organized in a P2P fashion and are capable of autonomously carrying out replication and recovery tasks. When one OSD suspects a peer as died or joined the cluster, it consults the Monitors to obtain an updated cluster map and possibly offloads some of his data to some other living OSD. The way an OSD knows based on the cluster map if he needs to send data to another OSD is the same process a client goes through to discover to which OSD he should write or read his data. In all cases, a client or OSD process runs the Ceph.CRUSH(placement groups, cluster state, rule set) to obtain the Target OSDs of the named object. The last two parameters are obtained directly from the monitors. The first parameter is the result of the operation: hash(object name) mod (size of object pool). CRUSH \cite{crush} is a light-weight pseudo-random placement algorithm, that provides deterministic object quantization without lookups to central directories. Because it runs independently on clients and peers, it avoids the need to have the monitor constantly updating cluster to object mappings.

Ceph is proof that cloud systems can leverage P2P behavior in order to dynamically adjust replication in a decentralized fashion while meeting high dependability standards. \cite{ceph, ceph_benchmarks}

\subsection{Structured Overlays}
\textbf{Kademlia} is structured overlay, yet it is fully decentralized, designed for key-value storage systems, which, according to the authors, combines provable consistency, performance, latency-minimizing routing, and symmetric topology \cite{kademlia}. Kademlia peer-identifiers and data-keys are built as in Chord\cite{chord}, using consistent hashing. Inserts and lookups on the overlay are unidirectional, according to the distance between two identifiers, calculated using XOR. The unidirectionality allows caching to be done along lookup paths, alleviating hot spots. Each peer in the overlay maintains a binary tree, routing table, whose leaves are lists of peers, called k-buckets, at distances $2^{x}$ to $2^{x+1}$, $x \in\hspace{1mm}\mathclose[0,160\mathclose]$ from itself, sorted according to a least recently seen policy. As a peer receives messages if the sender is a new acquaintance, that sender is placed on the proper k-bucket if there is enough space in it; otherwise, it will replace the least seen peer only if that peer fails to respond to a ping. By preferring old acquaintances, Kademlia seeks to enhance the stability of the overlay based on the observation that the longer a peer remains connected to the network, the higher the probability that it will remain connected at least another hour\cite{ssaroiu:msp2pfss}. This policy also prevents some denial of service attacks, where an enemy floods the network with new peers. The XOR metric gives Kademlia an advantage over other DHT-based implementations by making routing tables less rigid and by allowing peers in the network to learn useful routing information from discovery and lookup messages. Because of the existence of k-buckets, routing can also be done around failed nodes, providing some resistance to churn and partitioning.

The standard Kademlia implementation, however, does not implement an effective mechanism to keep dated entries out of a peer's k-buckets, meaning that the longer a peer stays offline, the worse is k-buckets are. Dated entries removed when the peer contacts a peer and obtains no response, meaning that insert and lookup operations may take longer than they could. The take-away from Kademlia protocol is that both the liveness and the freshness of an overlay can be improved by passively or actively making peers monitor each other.\newline

\textbf{Self-Chord} is a bio-inspired, structured overlay, specifically designed to be used in cloud computing or grid computing architectures. Self-chord foundations are that of the standard Chord, both in terms of identifiers and the way peers organize themselves on the overlay, i.e., in ring-like shapes. However, it has a few benefits, which are the result of having mobile agents, called ants, independently roaming the overlay network, but collaboratively re-arranging resource identifiers so that similar data items are clustered around neighboring peers with relatively proportional workloads, meaning that data item placement is not based solely on the numerical-space shared between them and possible placement peers \cite{sotart, selfchord}. Hence advantages of Self-Chord over the standard implementation include lack of need to assign data to well-specified peers since keys are independently defined, leading to better behavior under high churn conditions. Less overlay maintenance overhead since when a peer joins or leaves the network, ants will autonomously and eventually, distribute data items appropriately; Lastly, since load-balancing is continuously being carried out by ants, the scalability and robustness of the system increases since individual peer failures are less likely to lead to massive file loss. According to the authors, the behavior of the ants ensures that even under highly dynamic, unfavorable conditions, rearrangement, and discovery of data items takes only logarithmic time.

Bio-inspired systems like the one above, diminish or eliminate inherent disadvantages typically associated with structured or unstructured overlays, such as the ones mentioned in the introduction of this thesis proposal. In these, agents with simple behavior often accomplish complex behavior that would otherwise require time or space consuming algorithms, sometimes with high code complexity. In our proposal, we, too, seek to create a system that, just like Self-Chord, has self-healing and self-organizing behaviors.

\subsection{Unstructured Overlays}
\textbf{Gia} \cite{gia} is a decentralized overlay protocol whose purpose is to overcome the drawbacks of Gnutella\cite{gnutella-rfc, gnutella-cs}. Gnutella is the first-ever decentralized, unstructured P2P protocol. Gnutella's main issues are lack of scalability, as resource discovery bases itself on simple message flooding mechanisms and how easily some peers become overloaded when faced with high rates of aggregate queries, leading to system degradation. Gia proposes the use of super-nodes, which are now also part of the Gnutella protocol. The super-nodes receive and route queries to peers holding data, but these super-nodes and the construction of the topology around them is dynamic and adaptive. Gia recognizes the heterogeneity and resource constraints of the peers and also uses random walks to alter the topology adaptively as well as lookup operations. The results show that Gia is three to five orders of magnitude better than that of Gnutella. Random-walks by themselves, while minimizing the network overhead, are less likely to find appropriate responses to the performed lookup, unless they are biased to high-degree peers, but this can make peer overloading more frequent. Thus, to tackle both problems, Gia implements a topology adaptation that puts most peers within reach of high capacity peers, while simultaneously ensuring that they can handle the likely incoming requests. Lastly, all peers keep pointers to data items kept by their immediate one-hop neighbors.

The algorithm Gia uses to ensure high-capacity-peers are also high-degree ones depends on a satisfaction function that returns a value in $\mathclose[0,1\mathclose]$. As long as the returned value is not one, the peer takes the initiative of trying to connect itself to another peer at random, preferring those whose capacity is better than his. During the handshake, any of the intervenients may abort, based on their capacity and the degrees of their neighbors. During this process, the target of the handshake request accepts the requester whenever he does not know enough peers or the requester as more capacity than at least one of his neighbors. To avoid disconnecting poorly connected peers, the target of the handshake request will replace the highest capacity peer whose capacity is smaller than that of the requester. In order to avoid hot-spots or overloading of peers, a peer is only allowed to send requests to a neighbor if that neighbor has explicitly given him one Token; a token represents one request the neighbor is willing to accept; Each peer allocates tokens at the rate at which they can answer requests.

Gia successfully promotes optimal performance and longevity on the systems overlay topology through continuous optimization. Unlike many other unstructured overlays, Gia does not use message flooding to disseminate information, minimizing overhead. There is, however, one fundamental problem with Gia, related to the promotion of fairness; according to their capacity, peers, receive tokens from their neighbors, hence decreasing the usability of the system by low capacity peers. Either way, free riding is not a concept that directly applies to our proposal.\newline

\textbf{BlatAnt} \cite{blatant} is a bio-inspired, unstructured overlay, designed for grid computing architectures where resource discovery needs to be efficient; to this end, BlatAnt focuses solely on bounded-diameter optimization in order to minimize network overhead and lookup latency, by using swarm intelligence to support flood-like discovery protocols. Like in Self-Chord\cite{selfchord}, ant agents are used to optimize and maintain a self-structured P2P overlay, conversely, in BlatAnt the ants do not change the location of data items from peer to peer, but instead build and optimize the overlay such that the network uses a minimal number of connections between peers and such that the diameter of the overlay is bounded. The idea is that by doing so, the average number of hops, i.e., average path length, required to obtain a data item from a lookup will be smaller, thus achieving the sought goals. BlatAnt also uses local index caching to improve efficiency further; The authors argue that local caches,  as an alternative to semantic clustering such as the ones used in Antares\cite{antares} and Self-Chord, are better for self-organized overlays where a stable topology can not be guaranteed. The overlay management can be viewed in three major components.

The first component are the peers themselves. Each peer keeps two structures, a   fixed-size $\alpha$-table, i.e., a partial view of the overlay retaining neighborhood information used to evaluate the redundant connections or the need for new ones, and, a neighborhood-set \textit{N}, containing peer identifiers. Peers contribute to the optimization by rearranging local connections according to the connection rule, which reduces the diameter of the overlay, and disconnection rule, which discards redundant connections.

The second component are the ant agents, which have multiple species; \textit{discovery ants} are randomly spawned by peers and live for a limited time. They wander across the overlay, collecting information about its topology and update the $\alpha$-tables of peers they meet along the way. \textit{Construction ants} act as bootstrappers to the system. When a new peer \textit{i} wants to connect to a peer \textit{j}, he sends one of these ants, if \textit{j} cannot establish the connection because he would violate a degree constraint, he forwards the ant to his lowest-degree neighbor. When a \textit{j'} accepts the ant, he sends it back to \textit{i}, and the procedure complete, i.e., they are now on each others' neighborhood-sets. \textit{Optimization ants} establish connections according to the connection rule. In this case, a peer \textit{i} wanting to connect to peer \textit{j} sends him this ant, and \textit{j} only accepts or refuses the ant. If he accepts, he sends the ant back to \textit{i}. \textit{Unlink ants} remove existing connections between peers because of the disconnection rule applies or because a peer wants to leave the overlay. When this ant arrives at its destination, it removes all sender's information from the $\alpha$-table and the neighborhood-set. \textit{Update Neighbors Ants} are spawned by a peer whenever he establishes or tears-down a connection with some other. These ants visit all of that peer's neighbors and update his information on their respective $\alpha$-table. Finally, \textit{Ping Ants} are periodically exchanged between peers to keep their connections alive in low traffic situations.

The last component are pheromone trails, which evaporate overtime. A trail is a value assigned to a connection between peers, whenever an ant agent, or a query, walks over that connection, they increase the pheromone concentration increases on both ends. Discovery ants will tend to follow paths with less pheromone concentration, thus incentivizing full network coverage during the exploration. When a peer detects that for a given neighbor, its pheromone concentration has wholly evaporated, that neighbor is assumed to have left the network without warning. Eventually, each peer who neighbored the disconnected peer will independently initiate a recovery protocol by sending construction ants to the neighbors of the disconnected peer, reorganizing the overlay, and avoiding network partitioning. Thus pheromone trails provide seamless error resolution.

BlatAnt proved to accomplish its goals, quickly converging to stable overlays with bounded-diameter, even under very dynamic conditions \cite{sotart}, resulting in faster lookups due to the reduction of flood messages but also due to caching. Note that BlatAnt can leverage random walks such as the ones used in Gia for even less network overhead. BlatAnt also introduces optimizations over caching procedures; we do not review them here, because they are based on complex profile similarities and because caching is out of the scope of our proposal.\newline

\textbf{CYCLON} \cite{cyclon} is a protocol that emerges as an inexpensive solution to the maintenance of gossip-based unstructured overlays. CYCLON is useable even in highly dynamic environments, providing peer degree symmetry, low diameter, low clustering, resilience to churn and massive node failures, in a fully decentralized manner. One fundamental issue of unstructured networks is to avoid partitioning. One naive solution would be for each peer to keep a view of all the peers in the network and create connections has needed, but that solution is not scalable.

The protocol is simple. It consists of having each peer keeping an ever-changing, partial view, with fixed-size \textit{c}, of the network in a structure called \textit{neighbors}. Periodically and independently, a contacts a random neighbor to exchange acquaintances. This process is called \textit{shuffling}. During a shuffle, a peer \textit{i}, first increments the age of all his known neighbors by one. Then, selects the oldest of all his neighbors, \textit{j}, and $l-1$ other random neighbors, forming \textit{s}. Then \textit{i} replaces \textit{j}'s network address by his own and sets his age to zero, in \textit{s}. After that, \textit{i} sends \textit{s} to \textit{j} and waits for \textit{j} to reply with \textit{s'}. Unlike \textit{s}, \textit{s'} is composed of a purely random subset of \textit{j}'s neighbors, without any age modifications. Upon reception of subsets \textit{s} and \textit{s'}, \textit{j} and \textit{i}, respectively, update their \textit{neighbors} by first using any empty slots in their partial views and then by replacing entries which they sent to the other, i.e., if \textit{i} has no more slots, he replaces, in its \textit{neighbors} structure, the entries that he sent in \textit{s} to \textit{j}, with the ones he received in \textit{s'} from \textit{j}. Note that, after \textit{i} initiates a shuffle with \textit{j}, \textit{i} becomes \textit{j}'s neighbor, but \textit{j} is no longer a neighbor to \textit{i}, i.e., the neighboring relation between \textit{i} and \textit{j} is reversed. Unlike many protocols, CYCLON links between peers are not bidirectional.

The age is essential in the shuffling algorithm, for two reasons. First, it limits the lifetime of each peer in \textit{neighbors} structures, which globally bounds the number of existing pointers to them, it also limits the time each peers' addresses are passed around until they are selected as shuffling targets, resulting on a more up-to-date overlay as well as uniform distribution of each peers' addresses over the network.

When a peer leaves the overlay for any reason, the remaining peers may have to remove him from their \textit{neighbors} structure. Timely removal is fundamental for the robustness of the overlay.  CYCLON uses passive detection. Whenever a peer \textit{i} contacts \textit{j} for shuffling and obtains no response it assumes \textit{j} is disconnected and removes it from his \textit{neighbors}. Since the time for \textit{i} to contact \textit{j} is bounded by the age property in \textit{neighbors}, detection is accelerated.

CYCLON simulations show that the average path length and clustering coefficient, for various configurations, converged to values similar to those found in random graphs, i.e., converged to small-diameter topologies with low clustering. Gossip-based protocols often have high clustering, which is undesirable, since that may indicate an increased amount of redundant messages passing through the network and a higher chance of network partitioning. Another exciting result of CYCLON is that the average path length increased logarithmically, and clustering decreased exponentially, as the number of peers in the network increased, hence demonstrating scalability and increased robustness.

The authors also propose a join method that does not disrupt the randomness of the obtained overlays, while simultaneously making new peers indistinguishable from old ones. Whenever \textit{i} wishes to join the overlay, he contacts one peer \textit{j} already in the overlay who initiates \textit{c} random-walks with time-to-live (TTL) equal to the average path length of the overlay. The peer \textit{k} where the random-walk ends, replaces one of his \textit{neighbors} entries with \textit{i} network address, setting \textit{i} age set to zero and sends the replaced entry to \textit{i}. Note that even if some of the random walks do not complete due to byzantine failures, \textit{i} will remain connected to the overlay and will eventually find new acquaintances.

We conclude by reinforcing the idea that CYCLON is a scalable and robust solution for peer-sampling, providing good self-healing and self-organization capabilities without complex algorithms or apparent downsides. In a way, it relates to BlatAnt since the proposal passively bounds the diameter of the network.

\subsection{Cloud-Assisted P2P Protocol - CLOUDCAST}
\subsection{File Block Replication}
\subsection{Trust in Peer-to-Peer Networks}
\subsection{Probabilistic Swarm Guidance}
%%%%%%%%%%%%%%%%%%%%
% END RELATED WORK %
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN SOLUTION PROPOSAL %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Solution Proposal}\label{sec:proposal}
%%%%%%%%%%%%%%%%%%%%%%%%%
% END SOLUTION PROPOSAL %
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN EVALUATION METHODOLOGY %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation Methodology}\label{sec:methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END EVALUATION METHODOLOGY %
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN WORK SCHEDULE %
%%%%%%%%%%%%%%%%%%%%%%%
\section{Work Schedule}\label{sec:workschedule}
%%%%%%%%%%%%%%%%%%%%%
% END WORK SCHEDULE %
%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%
% BEGIN CONCLUSIONS %
%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%
% END CONCLUSIONS %
%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
% BEGIN REFERENCES %
%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs04}
\bibliography{bibliography}
%%%%%%%%%%%%%%%%%%
% END REFERENCES %
%%%%%%%%%%%%%%%%%%

\end{document}
%%%%%%%%%%%%%%%%
% END DOCUMENT %
%%%%%%%%%%%%%%%%
