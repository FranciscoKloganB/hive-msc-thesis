%% fsip2pnupsg.tex
%% V0.1
%% 2019/11/27
%% by Francisco Barros

%%%%%%%%%%%%%%%%%
% BEGIN IMPORTS %
%%%%%%%%%%%%%%%%%
\documentclass[runningheads]{llncs}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{optidef}
\usepackage{subfigure}
\usepackage{algorithmic}
\usepackage{algorithm}

\newcommand{\SubItem}[1]{{\setlength\itemindent{15pt} \item[-] #1}}
%%%%%%%%%%%%%%%
% END IMPORTS %
%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT %
%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%
% BEGIN HEADER %
%%%%%%%%%%%%%%%%
\title {Hives: File Survivability in P2P using Probabilistic Swarm Guidance
\thanks{This work has the support of the project MYRG2016-00097-FST of the University of Macau; by the Portuguese Fundação para a Ciência e a Tecnologia (FCT) through Institute for Systems and Robotics (ISR), under Laboratory for Robotics and Engineering Systems (LARSyS) project UID/EEA/50009/2019.}
}
\titlerunning{Hives}
\author{Francisco Barros, Daniel Silvestre \and Carlos Silvestre}
\authorrunning{F. Barros et al.}
\institute{Instituto Superior Técnico - Taguspark\newline Av. Prof. Doutor Cavaco Silva, 2744-016 Porto Salvo, Portugal
\email{fbarros@isr.ist.utl.pt, dsilvestre@isr.ist.utl.pt, csilvestre@umac.mo}\newline
\url{www.tecnico.ulisboa.pt}
}
\maketitle
%%%%%%%%%%%%%%
% END HEADER %
%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
% BEGIN ABSTRACT %
%%%%%%%%%%%%%%%%%%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in 150--250 words.
\begin{keywords}Agents-based Systems; Cooperative Control; Peer-to-Peer; Cloud Storage; Distributed control; File Availability; Swarm Guidance;\end{keywords}
\end{abstract}
%%%%%%%%%%%%%%%%
% END ABSTRACT %
%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN TABLE OF CONTENTS %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{tocdepth}{3}
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%
% END TABLE OF CONTENTS %
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% BEGIN INTRODUCTION %
%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Introduction}\label{sec:intro}

\subsection{Problem Description}
With the growth of the internet and the emergence of distributed systems, two large-scale computing paradigms have gained popularity due to their promise of virtually unlimited scalability. On the one hand, we have Peer-to-Peer (P2P) networking, which can be defined as a group of equally privileged peers, who contribute with a portion of their resources, to achieve common goals. These networks are popular among file sharing and streaming applications \cite{ssaroiu:msp2pfss}, due to their self-organized behavior, lack of centralization and, low-cost. On the other hand, Cloud platforms offer unmatched, on-demand, self-service, availability, and reliability, at a higher cost, and are currently trendy, with companies such as Amazon, Google, and Microsoft offering various ITaaS products to individuals and organizations alike. Cloud-based systems are centralized architectures, in which a large number of computers are clustered and managed by master entities, which possibly, become bottlenecks. Both paradigms can be used to deploy distributed file-storage or file-backup applications; despite the fact that P2P implementations are cheaper for both companies and their clients, these have a hard-time achieving 99.9\% availability seen Cloud-based implementations, furthermore, depending on the P2P application there may also exist the risk of permanent file loss, making them somewhat unappealing for clients who want to store or backup their files remotely.

\subsection{Our approach}
The focus of this thesis is to create a structured P2P overlay to be used on a distributed backup system, where clients upload their files to remote peers, who are selling their storage space, without persistence guarantees. Probabilistic Swarm Guidance (PSG) and Markov Chain theories \cite{markovchain_approach_to_pga} will be used, along with block-based file replication using erasure code algorithms and a reputation system, to hopefully create a network that is able to keep a file available at any given time, regardless of presence of attackers, network partitions and amount of churn, i.e., no matter how many peers leave/join an overlay.

\subsection
{Frequent approach}There has been much research on the topic of P2P overlays. There are two significant categories of overlays, Structured and Unstructured. In the former, the overlay has rigid rules regarding peer placement; they favor look-up speed but have higher maintenance overhead under churn, i.e., the process of peers leaving or joining the overlay. Some popular P2P Structured overlays include Chord \cite{chord}, Tapestry \cite{tapestry}, Kademlia \cite{kademlia}, and P-Grid \cite{pgrid}. In the latter, peer placement is random, which provides the overlay network with a higher degree of resilience and robustness in the advent of failures. Unstructured overlays disseminate information in a gossip-like fashion; thus, look-up operations are slower, resource management is less efficient, but overlay maintenance is often straightforward. Popular  Unstructured P2P overlays include FreeNet \cite{freenet}, Gnutella \cite{gnutella} and BitTorrent \cite{bittorrent}. Research on multi-layer overlays has not yielded satisfactory results, and implementation is difficult \cite{sotart}, but research on bio-inspired overlays did have promising results, some of these include Self-Chord \cite{selfchord}. Finally, there have also been some attempts of transparently integrating Cloud Systems into P2P networks, e.g., CLOUDCAST \cite{cloudcast}, and vice-versa, e.g., Spotify \cite{spotify} and Wuala \cite{wuala}. From all the mentioned research, perhaps the one that borrows most similarity with our proposal is CLOUDCAST; however, neither it nor any of the remaining literature, try to create a structured overlay composed of completely autonomous peers, who work together to keep the file alive without enforced persistence based on PSG.

\subsection{Proposal Goals}\label{subsec:intro}
\begin{itemize}
    \item Implement a Tracker server with light-weight responsibilities:
        \SubItem{Act as peer discovery service for clients;}
        \SubItem{Act as an indexer for clusters of peers (Hives);}
        \SubItem{Track peer reputations;}
    \item Implement an erasure-code algorithm to ensure file persists in the Hive;
    \item Implement a trust system where:
        \SubItem Each peer gives feedback to the Tracker about his neighbors;
        \SubItem Tracker synthesizes the feedback and presents it to the client;
    \item Calculate the ideal steady-state distribution of file blocks, for a Hive, when:
        \SubItem{No trust history on a new peer is available;}
        \SubItem{Peer availability, reliability and other trust factors are known;}
    \item Hives converge to ideal block distribution in bounded-times using PSG;
    \item Efficiently update the PSG's Markov Chain as it changes over time;
    \item Balance the in-degree and out-degree of each peer such that:
        \SubItem{Message exchange overhead is minimal;}
        \SubItem{Any changes to the probability of file loss are negligible;}
        \SubItem{Hive availability, reliability, and, confidentiality are not compromised;}
        \SubItem{Hive is resilient to massive node failures and resistant to churn;}
    \item Extra miles may include:
        \SubItem{Peers can reconstruct Hives without Tracker intervention;}
        \SubItem{Peers trust their neighbors independently of Tracker;}
        \SubItem{Hives can leverage dedicated Cloud-Storage if they become unreliable;}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%
% END INTRODUCTION %
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% BEGIN RELATED WORK %
%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Related Work}\label{sec:relatedwork}
This section as the following structure: In the first subsection, we will study two distributed file systems used in cloud architectures, in the following two subsections we will study some of the previously mentioned peer-to-peer, unstructured and structured, overlays. We will follow with considerations to have when choosing erasure code algorithms for file-block replication, two popular works on trust in peer-to-peer networks, and finally, we will mention one cloud-assisted P2P protocol that we consider relevant for being similar to the use-case that led us to study this thesis topic.
\subsection{Distributed File Systems}
\textbf{Google File System} (GFS) is designed to provide efficient and reliable access to data using clusters of commodity machines located in geographically spread data-centers. In particular, GFS aims to provide high data throughput, low latency, and reliability in the face of individual server failures. Because GFS runs over commodity hardware, the proposed model considers machine and network faults as being the norm rather than the exception \cite{gfs}.  This assumption is typical in modern distributed systems of any kind. GFS is the basis for an open-source project, which has seen continuous improvements over the years, called Hadoop Distributed File System (HDFS) \cite{hadoop}; we will not discuss it here because it holds many similarities to GFS with key differences being the approach to security management. One advantage of GFS over other architectures is that it leverages a distributed file system that tailors to the designers observation that, once written, files on google provided services, are only read, often sequentially, with random writes being practically non-existent, shifting optimization focus from latency to throughput, from random writes to appends when it comes to atomicity and no caching of data blocks \cite{gfs}. In the GFS architecture cluster consists of a single master and multiple chunk-servers. The Master maintains a mapping from files to chunks, the current location of chunks, and is responsible for failure detection over the chunk-servers as well as chunk-migration between them. Clients communicate, for reads and writes, with the Master for metadata operations and directly with the chunk-servers for data-bearing operations. Neither the client nor the chunk-servers cache file data because most of google's client applications stream through huge files. A read operation involves obtaining chunk handles and locations from the Master and then retrieving the chunks from one of the many available chunk-severs. For writes, the client splits the file into 64MB chunks and asks the Master to assign them an identifier, and then the client uploads his chunks to the chunk-servers indicated by the Master.

GFS has proven its qualities in supporting large-scale data processing. However, small files have shown \cite{gfs} to lead to overloads on chunk-servers holding them because chunks have a fixed large value size of 64MB. The reasoning for GFS to use such substantial value is to reduce network overload, reduce the number of accesses to the Master, and reduce memory used at the Master due to mappings. Concerning our proposal, Hives, since convergence to a desired steady-state using Markov Chains theory requires a large number of chunks to exist in a P2P Hive, we might have to do file-level replication rather than the desired block-level when client files are too small.\newline

\textbf{Ceph} is an open-source, storage system that, like GFS, runs over commodity hardware. Ceph aims to deliver reliable, autonomous, distributed object, block, and file storage that is self-healing, self-managing, and has no single point of failure. The base architectural component of Ceph is the RADOS object store, and all content a client reads and writes to the system are represented as objects, irrespective of the objects' original data type. RADOS is for all purposes a Ceph cluster, and its storage space originates from the disks of multiple machines. Ceph architecture is slightly more complicated than that of GFS, so we will summarize the components that are of interest to us. Inside a cluster, we find three types of machines, storage devices, abstracted with a layer called Object Software Daemon (OSD), Monitors, and Metadata Servers. A small count of Monitors maintain cluster membership and state using PAXOS to make consensual decisions. Metadata Servers only required sharing the distributed file system, are used for POSIX semantics, e.g., directory hierarchy and file metadata such as owner and last accessed timestamps. Monitors can detect if metadata servers are down and launch new ones as required. Finally, OSDs store clients' data, and there are anywhere between dozens to thousands of these in a Ceph cluster.

The innovation Ceph brings is that the OSDs within a cluster are organized in a P2P fashion and are capable of autonomously carrying out replication and recovery tasks. When one OSD suspects a peer as died or joined the cluster, it consults the Monitors to obtain an updated cluster map and possibly offloads some of his data to some other living OSD. The way an OSD knows based on the cluster map if he needs to send data to another OSD is the same process a client goes through to discover to which OSD he should write or read his data. In all cases, a client or OSD process runs the Ceph.CRUSH(placement groups, cluster state, rule set) to obtain the Target OSDs of the named object. The last two parameters are obtained directly from the monitors. The first parameter is the result of the operation: hash(object name) mod (size of object pool). CRUSH \cite{crush} is a light-weight pseudo-random placement algorithm, that provides deterministic object quantization without look-ups to central directories. Because it runs independently on clients and peers, it avoids the need to have the monitor constantly updating cluster to object mappings.

Ceph is proof that cloud systems can leverage P2P behavior in order to dynamically adjust replication in a decentralized fashion while meeting high dependability standards. \cite{ceph, ceph_benchmarks}

\subsection{Unstructured Overlays}
\textbf{Kademlia} Kademlia is the first P2P system to combine provable consistency, performance, latency-minimizing routing, and a symmetric topology. \cite{kademlia}
\subsection{Structured Overlays}
\subsection{File Block Replication}
\subsection{Trust in Peer-to-Peer Networks}
\subsection{Cloud-Assisted P2P Protocol - CLOUDCAST}
%%%%%%%%%%%%%%%%%%%%
% END RELATED WORK %
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN SOLUTION PROPOSAL %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Solution Proposal}\label{sec:proposal}
%%%%%%%%%%%%%%%%%%%%%%%%%
% END SOLUTION PROPOSAL %
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN EVALUATION METHODOLOGY %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation Methodology}\label{sec:methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END EVALUATION METHODOLOGY %
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN WORK SCHEDULE %
%%%%%%%%%%%%%%%%%%%%%%%
\section{Work Schedule}\label{sec:workschedule}
%%%%%%%%%%%%%%%%%%%%%
% END WORK SCHEDULE %
%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%
% BEGIN CONCLUSIONS %
%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%
% END CONCLUSIONS %
%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
% BEGIN REFERENCES %
%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs04}
\bibliography{bibliography}
%%%%%%%%%%%%%%%%%%
% END REFERENCES %
%%%%%%%%%%%%%%%%%%

\end{document}
%%%%%%%%%%%%%%%%
% END DOCUMENT %
%%%%%%%%%%%%%%%%
