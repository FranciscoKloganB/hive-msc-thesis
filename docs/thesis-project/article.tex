%% fsip2pnupsg.tex
%% V0.1
%% 2019/11/27
%% by Francisco Barros

%%%%%%%%%%%%%%%%%
% BEGIN IMPORTS %
%%%%%%%%%%%%%%%%%
\RequirePackage{amsmath}
\documentclass[runningheads]{llncs}
\usepackage{indentfirst}
\usepackage{cite}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{optidef}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{todonotes}

\newcommand{\SubItem}[1]{{\setlength\itemindent{15pt} \item[-] #1}}
%%%%%%%%%%%%%%%
% END IMPORTS %
%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT %
%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%
% BEGIN HEADER %
%%%%%%%%%%%%%%%%
\title {Hives: File Survivability in P2P using Probabilistic Swarm Guidance
\thanks{This work has the support of the project MYRG2016-00097-FST of the University of Macau; by the Portuguese Fundação para a Ciência e a Tecnologia (FCT) through Institute for Systems and Robotics (ISR), under Laboratory for Robotics and Engineering Systems (LARSyS) project UID/EEA/50009/2019.}
}
\titlerunning{Hives}
\author{Francisco Barros, Daniel Silvestre \and Carlos Silvestre}
\authorrunning{F. Barros et al.}
\institute{Instituto Superior Técnico - Taguspark\newline Av. Prof. Doutor Cavaco Silva, 2744-016 Porto Salvo, Portugal
\email{fbarros@isr.ist.utl.pt, dsilvestre@isr.ist.utl.pt, csilvestre@umac.mo}\newline
\url{www.tecnico.ulisboa.pt}
}
\maketitle
%%%%%%%%%%%%%%
% END HEADER %
%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
% BEGIN ABSTRACT %
%%%%%%%%%%%%%%%%%%
\begin{abstract}
Peer-to-Peer (P2P) systems have emerged as a potential choice to build large scale distributed storage systems, at a fraction of the cost of the alternative cloud approaches, which have better quality of service guarantees. We survey general-purpose P2P protocols, distributed file systems, cloud-assisted P2P protocols, and erasure-code algorithms. We then draft a solution for the problem of creating a bio-inspired P2P overlay to be used on a cloud-assisted P2P storage and backup system using probabilistic swarm guidance. We anticipate that our solution will have high network overhead, in order to provide self-healing properties to the files stored in the system.

\begin{keywords}Agents-based Systems; Cooperative Control; Peer-to-Peer; Cloud Storage; Distributed control; File Availability; Swarm Guidance;\end{keywords}
\end{abstract}
%%%%%%%%%%%%%%%%
% END ABSTRACT %
%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN TABLE OF CONTENTS %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{tocdepth}{3}
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%
% END TABLE OF CONTENTS %
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% BEGIN INTRODUCTION %
%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Introduction}\label{sec:intro}

\subsection{Problem Description}
With the growth of the internet and the emergence of distributed systems, two large-scale computing paradigms have gained popularity due to their promise of virtually unlimited scalability. On the one hand, we have Peer-to-Peer (P2P) networking, which can be defined as a group of equally privileged peers, who contribute with a portion of their resources, to achieve common goals. These networks are popular among file sharing and streaming applications \cite{ssaroiu:msp2pfss}, due to their self-organized behavior, lack of centralization and, low-cost. On the other hand, Cloud platforms offer unmatched, on-demand, self-service, availability, and reliability, at a higher cost, and are currently trendy, with companies such as Amazon, Google, and Microsoft offering various ITaaS products to individuals and organizations alike. Cloud-based systems are centralized architectures, in which a large number of computers are clustered and managed by master entities, which possibly, become bottlenecks. Both paradigms can be used to deploy distributed file-storage or file-backup applications; despite the fact that P2P implementations are cheaper for both companies and their clients, these have a hard-time achieving 99.9\% availability seen in Cloud-based implementations, furthermore, depending on the P2P application there may also exist the risk of permanent file loss, making them somewhat unappealing for clients who want to store or backup their files remotely.

\subsection{Our approach}
The focus of this thesis is to create a structured P2P overlay to be used on a distributed backup system, where clients upload their files to remote peers, who are selling their storage space, without persistence guarantees. Probabilistic Swarm Guidance (PSG) and Markov Chain theories will be used, along with block-based file replication using erasure code algorithms and a reputation system, to hopefully create a network that is able to keep a file available at any given time, regardless of presence of attackers, network partitions and amount of churn, i.e., no matter how many peers leave/join an overlay.

\subsection
{Frequent approach}There has been much research on the topic of P2P overlays. There are two significant categories of overlays, Structured and Unstructured. In the former, the overlay has rigid rules regarding peer placement; they favor lookup speed but have higher maintenance overhead under churn, i.e., the process of peers leaving or joining the overlay. Some popular P2P Structured overlays include Chord \cite{chord}, Tapestry \cite{tapestry}, Kademlia \cite{kademlia}, and P-Grid \cite{pgrid}. In the latter, peer placement is random, which provides the overlay network with a higher degree of resilience and robustness in the advent of failures. Unstructured overlays disseminate information in a gossip-like fashion; thus, lookup operations are slower, resource management is less efficient, but overlay maintenance is often straightforward. Popular  Unstructured P2P overlays include FreeNet \cite{freenet}, Gnutella \cite{gnutella-rfc} and BitTorrent \cite{bittorrent}. Research on multi-layer overlays has not yielded satisfactory results, and implementation is difficult \cite{sotart}, but research on bio-inspired overlays did have promising results, some of these include Self-Chord \cite{selfchord}. Finally, there have also been some attempts of transparently integrating Cloud Systems into P2P networks, e.g., CLOUDCAST \cite{cloudcast}, and vice-versa, e.g., Spotify \cite{spotify} and Wuala \cite{wuala}. From all the mentioned research, perhaps the one that borrows most similarity with our proposal is CLOUDCAST; however, neither it nor any of the remaining literature we present here, try to create a structured overlay composed of completely autonomous peers, who work together to keep the file at all points in time using PSG.

\subsection{Proposal Goals}\label{subsec:intro}
This work seeks to address the problem of file survivability in P2P networks; our primary goal is to study, design, and evaluate a P2P overlay that uses probabilistic swarm guidance as a way to maintain a file available to service clients at all times. Steps will include:

\begin{itemize}
    \item Implement a Tracker server with light-weight responsibilities:
        \SubItem{Act as peer discovery service for clients;}
        \SubItem{Act as an indexer for clusters of peers (Hives);}
        \SubItem{Track peer reputations;}
    \item Implement an erasure-code algorithm used by Hives to replicate clients' files;
    \item Calculate the ideal steady-state distribution of file blocks, for a Hive, when:
        \SubItem{No history information on a peer is available;}
        \SubItem{Peer availability, reliability and other trust factors are known;}
    \item Hives converge to ideal block distribution in bounded-times using PSG;
    \item Efficiently update routing as Hives' topologies change over time;
    \item Balance the degree of each peer such that:
        \SubItem{Message exchange overhead is minimal;}
        \SubItem{Any changes to the probability of file loss are negligible;}
        \SubItem{Hive is resilient to massive node failures and resistant to churn;}
    \item Evaluating the system using PeerSim \cite{peersim} or equivalent platform;
    \item Extra miles may include:
        \SubItem{Implement a trust system managed by the Tracker}
        \SubItem{Peers can reconstruct Hives without Tracker intervention;}
        \SubItem{Peers trust their neighbors independently of Tracker;}
        \SubItem{Hives can leverage dedicated Cloud-Storage if they become unreliable;}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%
% END INTRODUCTION %
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% BEGIN RELATED WORK %
%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Distributed File Systems}\label{sec:distributed-file-systems}
Distributed file system (DFS) enables individuals and organizations to store, access, and secure data on possibly remote locations, precisely as they would their local machines. It is common practice to replicate data at the file-level, block-level, or both, depending on the requirements of each system. For example, block-level replication may allow parallel execution of applications, such as is the case of Hadoop, Google, and Spark MapReduce algorithms. Here we present two DFS, which follow entirely different approaches when it comes to architecture and protocols. We focus only on the architectural implementation, their advantages or disadvantages. We do not consider parallel computation because our primary objective, in this thesis proposal, is the continuous availability of data in a system, to clients. Before we dive into them, it is essential to understand some desired characteristics of a DFS. In general, distributed systems of any kind should be \textbf{\textit{dependable}}, meaning that they should be able to avoid service failures that are more frequent and more severe than is acceptable. To be dependable is to be ready to provide the correct service (\textbf{\textit{availability}}), in a continuous manner (\textbf{\textit{reliability}}), with absence of catastrophic consequences to clients (\textbf{\textit{safety}}), e.g., losing a file, with no improper system alterations (\textbf{\textit{integrity}}) and, finally, to be able to undergo modifications and repairs at any time (\textbf{\textit{maintainability}}). Some of the features that contribute to the above characteristics are summarized below.

\textbf{\textit{Load-Balancing}} is the process of balancing communication, processing, and storage overhead among all nodes of the system. It helps to avoid bottlenecks, increasing operational performance, and helps to prevent catastrophic failures. Prevention of failures is also achieved through \textbf{\textit{fault tolerance}}, which enables a system to continue operating, possibly at a reduced level,  rather  than failing completely, when some part of it fails, to this end, some sort of \textbf{\textit{machine redundancy}} and \textbf{\textit{data redundancy}} may be important. As the amount of data passing in the network, being processed or stored in the system grows, or the number of clients using it increases, the capacity of a system to maintain its service without, or with little degradation, is called \textbf{\textit{scalability}}, it depends on the implementation of the system, and as a rule of thumb, centralized solutions are less scalable.

\subsection{Google File System}

Google File System (GFS) \cite{gfs} is designed to provide efficient and reliable access to data using clusters of commodity machines located in geographically spread data-centers. In particular, GFS aims to provide high data throughput, low latency, and reliability in the face of individual server failures. Because GFS runs over commodity hardware, the proposed model considers machine and network faults as being the norm rather than the exception \cite{gfs}.  This assumption is typical in modern distributed systems of any kind. GFS is the basis for an open-source project, which has seen continuous improvements over the years, called Hadoop Distributed File System (HDFS) \cite{hadoop}; we will not discuss it here because it holds many similarities to GFS with key differences being the approach to security management. One advantage of GFS over other architectures is that it leverages a distributed file system that tailors to the designers observation that, once written, files on google provided services, are only read, often sequentially, with random writes being practically non-existent, shifting optimization focus from latency to throughput, from random writes to appends when it comes to atomicity and no caching of data blocks \cite{gfs}. In the GFS architecture cluster consists of a single master and multiple chunk-servers. The Master maintains a mapping from files to chunks, the current location of chunks, and is responsible for failure detection over the chunk-servers as well as chunk-migration between them. Clients communicate, for reads and writes, with the Master for metadata operations and directly with the chunk-servers for data-bearing operations. Neither the client nor the chunk-servers cache file data because most of google's client applications stream through huge files. A read operation involves obtaining chunk handles and locations from the Master and then retrieving the chunks from one of the many available chunk-severs. For writes, the client splits the file into 64MB chunks and asks the Master to assign them an identifier, and then the client uploads his chunks to the chunk-servers indicated by the Master. See Fig.\ref{fig:gfs_arch} for a simplified architecture diagram.

GFS has proven its qualities in supporting large-scale data processing. However, small files have shown \cite{gfs} to lead to overloads on chunk-servers holding them because chunks have a fixed large value size of 64MB. The reasoning for GFS to use such substantial value is to reduce network overload, reduce the number of accesses to the Master, and reduce memory used at the Master due to mappings. Concerning our proposal, Hives, since convergence to a desired steady-state using Markov Chains theory requires a large number of chunks to exist in a P2P Hive, we might have to do file-level replication rather than the desired block-level when client files are too small.\newline

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{images/gfs_paper.png}
\caption{GFS Architecture \cite{gfs}}
\label{fig:gfs_arch}
\end{figure}

\newpage\subsection{Ceph File System}
Ceph \cite{ceph} is an open-source, storage system that, like GFS, runs over commodity hardware. Ceph aims to deliver reliable, autonomous, distributed object, block, and file storage that is self-healing, self-managing, and has no single point of failure. The base architectural component of Ceph is the \textit{RADOS} object store. All content clients read and write to the system is represented as an object, irrespective of the object's original data type. \textit{RADOS} is a single logical entity composed of many \textit{Ceph Storage clusters}. Ceph architecture, Fig. \ref{fig:cfs_arch} is slightly more sophisticated than that of GFS, so we will summarize the components that are of interest to us. Inside a \textit{Ceph Storage cluster}, we find four types of logical machines, storage devices, abstracted with a layer called \textit{Object Software Daemon} (OSD), \textit{Monitors}, \textit{Managers}, and \textit{Metadata Servers}. \textit{Metadata Servers} are only required when a client his sharing his DFS with other clients, these servers contain information used for POSIX semantics, e.g., file metadata such as owner and last accessed timestamps. A small amount of \textit{Monitors} maintain a master copy of the storage cluster map with its current state. Monitors require high consistency and use Paxos \cite{paxos} to decide on the state of the cluster. \textit{Managers} maintains detailed information about placement groups, process metadata and host metadata. Finally, OSD store data on behalf of clients. Additionally, OSD utilize their nodes' resources to perform data replication, erasure coding, load-balancing, recovery, monitoring and reporting functions. Each Storage Cluster as anywhere from dozens to thousands of these OSDs.

The innovation Ceph brings is that the OSD within the same \textit{placement group}, within a cluster, are organized in a P2P fashion and are capable of autonomously carrying out replication and recovery tasks. \textit{Placement groups} are fragments of a logical \textit{pool}, and in turn, \textit{pools} are logical, dynamic partitions that define among other things, how much and which type of replication is done to objects that belong to it. When one OSD in a placement group suspects another as died or joined the cluster, it consults a \textit{Monitor} to obtain an up to date cluster map. Based on the cluster map and the \textit{CRUSH}\cite{crush} algorithm, it is possible that this OSD will have to offload some of its content to other OSD. Clients also use \textit{CRUSH} to perform reads and writes to the system. To perform a write operation, for example, a client connects to a \textit{pool} in the storage cluster and runs \textit{CRUSH} with a cluster map retrieved from a Monitor. The result of the operation gives the client information on the placement group of the data he wishes to upload as well as the OSD he should contact to complete his request. When the target OSD receives the request, it runs \textit{CRUSH} to find the number of replicas that it should store. Then, it takes the object identifier, pool name, and the cluster map and uses \textit{CRUSH}, again, to discover the secondary OSDs to where he will replicate the write. The operation is completed when the primary OSD, receives acknowledgments from the remaining secondary OSD and acknowledges back to the client. \textit{CRUSH} is a light-weight pseudo-random placement algorithm that provides deterministic object quantization without lookups to central directories. Because it runs independently on clients and peers, it avoids the need to have the monitor constantly updating cluster to object mappings. Ceph is proof that distributed file systems can reliably be implemented using P2P behavior. Ceph OSDs perform a lot of self monitoring and management operations automatically and in a decentralized fashion, while being highly dependable. \cite{ceph, ceph_benchmarks}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\textwidth]{images/cephfs.png}
\caption{CFS Architecture \cite{configure-ceph}}
\label{fig:cfs_arch}
\end{figure}

\newpage\section{P2P Overlays}\label{sec:p2p-overlays}

In large-scale P2P systems, it is unfeasible and even undesirable for each peer in the network to know and explicitly interact with every other node connecting it to other peers. In this regard, a P2P overlay is a logical network that abstracts the physical network underneath it, typically IP-based, and where, single-hop edges represent links between pairs of peers, as depicted in Fig. \ref{fig:network_diagrams}. Before we survey related work in the field of P2P overlays, we first list some essential properties and technical jargon related to overlays and graph modeling, that helps us understand better the pros and cons of the research we are presenting in the next subsections of this thesis proposal.

\begin{figure}[ht]
    \centering
    \subfigure{\label{fig:iplayer}\includegraphics[width=0.35\textwidth, clip=true]{images/ip-layer.png}}
    \hspace{0.2\textwidth}
    \subfigure{\label{fig:overlay}\includegraphics[width=0.35\textwidth, clip=true]{images/overlay-layer.png}}
    \caption{P2P Network Diagrams: IP layer \textit{vs.} One possible overlay abstraction}
    \label{fig:network_diagrams}
\end{figure}

An overlay is said to be \textbf{\textit{connected}} if there is a path that allows any pair of peers \textit{i}, \textit{j} to communicate with each other, i.e., there is no partitioning and no peer is isolated in the system.

The \textbf{\textit{degree}} of a peer is the total number of connections established between some peer \textit{i} and other peers. It is possible to divide the degree into \textit{in-degree} and \textit{out-degree}, i.e., the number of channels a peer \textit{i} has, through which it can only receive data or send data, respectively. When all peers in an overlay have the same degree, the overlay is said to be \textbf{\textit{regular}} or \textbf{\textit{symmetric}}, but this is not usual. We can use the \textbf{\textit{degree distribution}} to evaluate the robustness of an overlay since through it; it is possible to detect \textit{weakly connected peers} and \textit{massively connected hubs}, i.e., peers with very high degree. In general, we want to minimize the existence of hubs. In P2P networks, hubs may provide the overlay with resistance against random damage and possibly speed-up lookup operations in unstructured overlays, using biased searching; however, targeted-attacks to hubs, or natural hub failures will have more significant impacts on the connectivity of the overlay, possibly leading to partitioning or halting of the system \cite{webdragons, controlling_the_hubs}. Peer degrees can also allow evaluation of fair work distribution among peers of the system, e.g., in terms of processing or bandwidth.

\textbf{\textit{Network diameter}} is the number of links that compose the longest of all the shortest paths between any two peers \textit{i}, \textit{j}. Knowing the length of all shortest paths between all possible pairs of peers \textit{i}, \textit{j} one can extract the \textbf{\textit{average path length}} and hence derive how messages, on average, will be relayed on the overlay to contact a stranger. Still regarding distances, the path stretch is the ratio between the number of links a message has to traverse, between any two peers, in the underlying network and the overlay network.

\textbf{\textit{Clustering}} directly affects connectivity and network diameter; we can measure it using the \textbf{\textit{clustering coefficient}}. This coefficient represents the percentage of neighbors a peer \textit{i} has, who are also neighbors among themselves. Like peers with high degrees, high clustering coefficients indicate the increased chance of partitioning in the system in the event of massive peer failures or high degrees of \textit{churn}. Furthermore, high clustering coefficients may indicate, in gossip-based protocols, bandwidth overhead due to redundant communications in the cluster, but has the advantage of making lookups faster even when the network is experiencing heavy load, e.g., flash crowd scenarios.

Another two terms that often appear in P2P literature are the small-world and the power-law properties. The small-world property is an umbrella term for a system with a high clustering coefficient and low average path length, yet most peers are not direct neighbors of each other. The power-law property means that there is non-uniform distribution influence in the overlay, e.g., some peers have more responsibilities or perks because they have higher capacities than the rest.

\subsection{Structured Overlays}
\subsubsection{Kademlia} \cite{kademlia} is structured overlay, yet it is fully decentralized, designed for key-value storage systems, which, according to the authors, combines provable consistency, performance, latency-minimizing routing, and symmetric topology \cite{kademlia}. Kademlia peer-identifiers and data-keys are built as in Chord\cite{chord}, using consistent hashing. Inserts and lookups on the overlay are unidirectional, according to the distance between two identifiers, calculated using XOR. The unidirectionality allows caching to be done along lookup paths, alleviating hot spots. Each peer in the overlay maintains a binary tree, routing table, whose leaves are lists of peers, called k-buckets, at distances $2^{x}$ to $2^{x+1}$, $x \in\hspace{1mm}\mathclose[0,160\mathclose]$ from itself, sorted according to a least recently seen policy. As a peer receives messages if the sender is a new acquaintance, that sender is placed on the proper k-bucket if there is enough space in it; otherwise, it will replace the least seen peer only if that peer fails to respond to a ping. By preferring old acquaintances, Kademlia seeks to enhance the stability of the overlay based on the observation that the longer a peer remains connected to the network, the higher the probability that it will remain connected at least another hour\cite{ssaroiu:msp2pfss}. This policy also prevents some denial of service attacks, where an enemy floods the network with new peers. The XOR metric gives Kademlia an advantage over other DHT-based implementations by making routing tables less rigid and by allowing peers in the network to learn useful routing information from discovery and lookup messages. Because of the existence of k-buckets, routing can also be done around failed nodes, providing some resistance to churn and partitioning.

The standard Kademlia implementation, however, does not implement an effective mechanism to keep dated entries out of a peer's k-buckets, meaning that the longer a peer stays offline, the worse is k-buckets are. Dated entries removed when the peer contacts a peer and obtains no response, meaning that insert and lookup operations may take longer than they could. The take-away from Kademlia protocol is that both the liveness and the freshness of an overlay can be improved by passively or actively making peers monitor each other.\newline

\subsubsection{Self-Chord} \cite{selfchord} is a bio-inspired, structured overlay, specifically designed to be used in cloud computing or grid computing architectures. Self-chord foundations are that of the standard Chord, both in terms of identifiers and the way peers organize themselves on the overlay, i.e., in ring-like shapes. However, it has a few benefits, which are the result of having mobile agents, called ants, independently roaming the overlay network, but collaboratively re-arranging resource identifiers so that similar data items are clustered around neighboring peers with relatively proportional workloads, meaning that data item placement is not based solely on the numerical-space shared between them and possible placement peers \cite{sotart, selfchord}. Hence advantages of Self-Chord over the standard implementation include lack of need to assign data to well-specified peers since keys are independently defined, leading to better behavior under high churn conditions. Less overlay maintenance overhead since when a peer joins or leaves the network, ants will autonomously and eventually, distribute data items appropriately; Lastly, since load-balancing is continuously being carried out by ants, the scalability and robustness of the system increases since individual peer failures are less likely to lead to massive file loss. According to the authors, the behavior of the ants ensures that even under highly dynamic, unfavorable conditions, rearrangement, and discovery of data items takes only logarithmic time.

Bio-inspired systems like the one above, diminish or eliminate inherent disadvantages typically associated with structured or unstructured overlays, such as the ones mentioned in the introduction of this thesis proposal. In these, agents with simple behavior often accomplish complex behavior that would otherwise require time or space consuming algorithms, sometimes with high code complexity. In our proposal, we, too, seek to create a system that, just like Self-Chord, has self-healing and self-organizing behaviors.

\newpage\subsection{Unstructured Overlays}
\subsubsection{Gia} \cite{gia} is a decentralized overlay protocol whose purpose is to overcome the drawbacks of Gnutella\cite{gnutella-rfc, gnutella-cs}. Gnutella is the first-ever decentralized, unstructured P2P protocol. Gnutella's main issues are lack of scalability, as resource discovery bases itself on simple message flooding mechanisms and how easily some peers become overloaded when faced with high rates of aggregate queries, leading to system degradation. Gia proposes the use of super-nodes, which are now also part of the Gnutella protocol. The super-nodes receive and route queries to peers holding data, but these super-nodes and the construction of the topology around them is dynamic and adaptive. Gia recognizes the heterogeneity and resource constraints of the peers and also uses random walks to alter the topology adaptively as well as lookup operations. The results show that Gia is three to five orders of magnitude better than that of Gnutella. Random-walks by themselves, while minimizing the network overhead, are less likely to find appropriate responses to the performed lookup, unless they are biased to high-degree peers, but this can make peer overloading more frequent. Thus, to tackle both problems, Gia implements a topology adaptation that puts most peers within reach of high capacity peers, while simultaneously ensuring that they can handle the likely incoming requests. Lastly, all peers keep pointers to data items kept by their immediate one-hop neighbors.

The algorithm Gia uses to ensure high-capacity-peers are also high-degree ones depends on a satisfaction function that returns a value in $\mathclose[0,1\mathclose]$. As long as the returned value is not one, the peer takes the initiative of trying to connect itself to another peer at random, preferring those whose capacity is better than his. During the handshake, any of the intervenients may abort, based on their capacity and the degrees of their neighbors. During this process, the target of the handshake request accepts the requester whenever he does not know enough peers or the requester as more capacity than at least one of his neighbors. To avoid disconnecting poorly connected peers, the target of the handshake request will replace the highest capacity peer whose capacity is smaller than that of the requester. In order to avoid hot-spots or overloading of peers, a peer is only allowed to send requests to a neighbor if that neighbor has explicitly given him one Token; a token represents one request the neighbor is willing to accept; Each peer allocates tokens at the rate at which they can answer requests.

Gia successfully promotes optimal performance and longevity on the systems overlay topology through continuous optimization. Unlike many other unstructured overlays, Gia does not use message flooding to disseminate information, minimizing overhead. There is, however, one fundamental problem with Gia, related to the promotion of fairness; according to their capacity, peers, receive tokens from their neighbors, hence decreasing the usability of the system by low capacity peers. Either way, free riding is not a concept that directly applies to our proposal.\newline

\subsubsection{BlatAnt} \cite{blatant} is a bio-inspired, unstructured overlay, designed for grid computing architectures where resource discovery needs to be efficient; to this end, BlatAnt focuses solely on bounded-diameter optimization in order to minimize network overhead and lookup latency, by using swarm intelligence to support flood-like discovery protocols. Like in Self-Chord\cite{selfchord}, ant agents are used to optimize and maintain a self-structured P2P overlay, conversely, in BlatAnt the ants do not change the location of data items from peer to peer, but instead build and optimize the overlay such that the network uses a minimal number of connections between peers and such that the diameter of the overlay is bounded. The idea is that by doing so, the average number of hops, i.e., average path length, required to obtain a data item from a lookup will be smaller, thus achieving the sought goals. BlatAnt also uses local index caching to improve efficiency further; The authors argue that local caches,  as an alternative to semantic clustering such as the ones used in Antares\cite{antares} and Self-Chord, are better for self-organized overlays where a stable topology can not be guaranteed. The overlay management can be viewed in three major components.

The first component are the peers themselves. Each peer keeps two structures, a   fixed-size $\alpha$-table, i.e., a partial view of the overlay retaining neighborhood information used to evaluate the redundant connections or the need for new ones, and, a neighborhood-set \textit{N}, containing peer identifiers. Peers contribute to the optimization by rearranging local connections according to the connection rule, which reduces the diameter of the overlay, and disconnection rule, which discards redundant connections.

The second component are the ant agents, which have multiple species; \textit{discovery ants} are randomly spawned by peers and live for a limited time. They wander across the overlay, collecting information about its topology and update the $\alpha$-tables of peers they meet along the way. \textit{Construction ants} act as bootstrappers to the system. When a new peer \textit{i} wants to connect to a peer \textit{j}, he sends one of these ants, if \textit{j} cannot establish the connection because he would violate a degree constraint, he forwards the ant to his lowest-degree neighbor. When a \textit{j'} accepts the ant, he sends it back to \textit{i}, and the procedure complete, i.e., they are now on each others' neighborhood-sets. \textit{Optimization ants} establish connections according to the connection rule. In this case, a peer \textit{i} wanting to connect to peer \textit{j} sends him this ant, and \textit{j} only accepts or refuses the ant. If he accepts, he sends the ant back to \textit{i}. \textit{Unlink ants} remove existing connections between peers because of the disconnection rule applies or because a peer wants to leave the overlay. When this ant arrives at its destination, it removes all sender's information from the $\alpha$-table and the neighborhood-set. \textit{Update Neighbors Ants} are spawned by a peer whenever he establishes or tears-down a connection with some other. These ants visit all of that peer's neighbors and update his information on their respective $\alpha$-table. Finally, \textit{Ping Ants} are periodically exchanged between peers to keep their connections alive in low traffic situations.

The last component are pheromone trails, which evaporate overtime. A trail is a value assigned to a connection between peers, whenever an ant agent, or a query, walks over that connection, they increase the pheromone concentration increases on both ends. Discovery ants will tend to follow paths with less pheromone concentration, thus incentivizing full network coverage during the exploration. When a peer detects that for a given neighbor, its pheromone concentration has wholly evaporated, that neighbor is assumed to have left the network without warning. Eventually, each peer who neighbored the disconnected peer will independently initiate a recovery protocol by sending construction ants to the neighbors of the disconnected peer, reorganizing the overlay, and avoiding network partitioning. Thus pheromone trails provide seamless error resolution.

BlatAnt proved to accomplish its goals, quickly converging to stable overlays with bounded-diameter, even under very dynamic conditions \cite{sotart}, resulting in faster lookups due to the reduction of flood messages but also due to caching. Note that BlatAnt can leverage random walks such as the ones used in Gia for even less network overhead. BlatAnt also introduces optimizations over caching procedures; we do not review them here, because they are based on complex profile similarities and because caching is out of the scope of our proposal.\newline

\section{Membership Management Frameworks}\label{sec:membership-management}

Gossip-based protocols, also know has epidemic protocols, gained popularity in the nineties for their usability to solve problems like database replication, failure detection, and resource-monitoring. In this section, we present two such protocols used for membership management, i.e., used to keep current partial views of large-scale, dynamic networks, built on unstructured overlays. Just like we mentioned in section \ref{sec:p2p-overlays}, keeping a view of all peers in the system is not a solution. Another fundamental issue of unstructured networks is to avoid partitioning. The following alternatives are both scalable, robust, and decentralized solutions to both these problems. Likewise, peers act autonomously in order to provide self-organization or self-healing capabilities to the overlays in which the protocols are applied.

\subsection{CYCLON} CYCLON\cite{cyclon} is useable even in highly dynamic environments, providing peer degree symmetry, low diameter, low clustering, resilience to churn, and massive node failures. The focus of the authors was to provide a lightweight and simple protocol. It consists of having each peer keeping an ever-changing, partial view, with fixed-size \textit{c}, of the network in a structure called \textit{neighbors}. Periodically, independently and asynchronously, a peer contacts a random neighbor to exchange acquaintances. This process is called \textit{shuffling}. During a shuffle, a peer \textit{i}, first increments the age of all his known neighbors by one. Then, selects the oldest of all his neighbors, \textit{j}, and $l-1$ other random neighbors, forming \textit{s}. Then \textit{i} replaces \textit{j}'s network address by his own and sets his age to zero, in \textit{s}. After that, \textit{i} sends \textit{s} to \textit{j} and waits for \textit{j} to reply with \textit{s'}. Unlike \textit{s}, \textit{s'} is composed of a purely random subset of \textit{j}'s neighbors, without any age modifications. Upon reception of subsets \textit{s} and \textit{s'}, \textit{j} and \textit{i}, respectively, update their \textit{neighbors} by first using any empty slots in their partial views and then by replacing entries which they sent to the other, i.e., if \textit{i} has no more slots, he replaces, in its \textit{neighbors} structure, the entries that he sent in \textit{s} to \textit{j}, with the ones he received in \textit{s'} from \textit{j}. Note that, after \textit{i} initiates a shuffle with \textit{j}, \textit{i} becomes \textit{j}'s neighbor, but \textit{j} is no longer a neighbor to \textit{i}, i.e., the neighboring relation between \textit{i} and \textit{j} is reversed. Unlike many protocols, CYCLON links between peers are not bidirectional.

The age is essential in the shuffling algorithm, for two reasons. First, it limits the lifetime of each peer in \textit{neighbors} structures, which globally bounds the number of existing pointers to them, it also limits the time each peers' addresses are passed around until they are selected as shuffling targets, resulting on a more up-to-date overlay as well as uniform distribution of each peers' addresses over the network.

When a peer leaves the overlay for any reason, the remaining peers may have to remove him from their \textit{neighbors} structure. Timely removal is fundamental for the robustness of the overlay.  CYCLON uses passive detection. Whenever a peer \textit{i} contacts \textit{j} for shuffling and obtains no response it assumes \textit{j} is disconnected and removes it from his \textit{neighbors}. Since the time for \textit{i} to contact \textit{j} is bounded by the age property in \textit{neighbors}, detection is accelerated.

CYCLON simulations show that the average path length and clustering coefficient, for various configurations, converged to values similar to those found in random graphs, i.e., converged to small-diameter topologies with low clustering. Gossip-based protocols often have high clustering, which is undesirable both in terms of robustness and overhead. Another exciting result of CYCLON is that the average path length increased logarithmically, and clustering decreased exponentially, as the number of peers in the network increased, hence demonstrating scalability and increased robustness.

The authors also propose a join method that does not disrupt the randomness of the obtained overlays, while simultaneously making new peers indistinguishable from old ones. Whenever \textit{i} wishes to join the overlay, he contacts one peer \textit{j} already in the overlay who initiates \textit{c} random-walks with time-to-live (TTL) equal to the average path length of the overlay. The peer \textit{k} where the random-walk ends, replaces one of his \textit{neighbors} entries with \textit{i} network address, setting \textit{i} age set to zero and sends the replaced entry to \textit{i}. Note that even if some of the random walks do not complete due to byzantine failures, \textit{i} will remain connected to the overlay and will eventually find new acquaintances.

We conclude by reinforcing that CYCLON provides good self-healing and self-organization capabilities without complex algorithms or apparent downsides other than the one where too few peers in the system may cause it to be under performant due to clustering effects. In a way, it relates to BlatAnt since the proposal passively bounds the diameter of the network.

\newpage\subsection{Newscast}
In Newscast \cite{newscast-computing}, all peers proactively exchange, timestamped, information with each other, including not only neighboring information but also app-specific information, allowing for simplified implementation of aggregation algorithms, i.e., finding statistics regarding for example network size. Newscast is responsible for membership-management and a second function, which does not exist in CYCLON, which is information dissemination.

In Newscast, there is a separation of responsibilities. Within each peer, there are two types of entities. Those who run the newscast protocol, are called \textit{corespondents}. Those who run processes of a distributed application are called \textit{agents}. \textit{Correspondents} run the same instance of the newscast protocol. \textit{Agents} are not required to run the same application. Each \textit{agent} has one or more \textit{correspondents}. This separation does not explicitly exist in CYCLON. Each \textit{agent} implements an interface with two functions \textit{getNews()} and \textit{newsUpdate(news[])}. The \textit{correspondent} uses the former to request news from their \textit{agent}, and the latter to deliver app-specific news collected from other peers' \textit{correspondents} in the network. Each \textit{correspondent} as a fixed-size news cache of size \textit{c} and periodically exchange news with each other has followed: request news to the \textit{agent}, timestamp them along with the local time, and IP-address of the \textit{agent}, then storing them in a cache entry. Afterward, it selects another peer's IP address running the same \textit{correspondent} aggregation function, as available in his cache. Both \textit{correspondents} exchange the entirety of their caches and pass the merged cache with $2c+1$ entries to their respective \textit{agents} before discarding the oldest entries, after timestamp normalization, down to \textit{c} and storing the results themselves. The timestamp normalization is not a clock-synchronization process and has some inaccuracy, but as long as transmission times between two peers are not too big, there should be no problem. During the discarding process, the algorithm ensures that there is at most one news item per \textit{agent}.

It is debatable if Newscast has an advantage over CYCLON since news exchange messages already include peer addresses, leading to a transparent resolution of the membership problem. CYCLON, however, has several advantages over Newscast. These include: the resulting overlays of CYCLON have better randomness due to its shuffling-based algorithm, whereas Newscast and other gossip-based management frameworks, seem to have small-world properties according to previous research \cite{eval-gossip-based}. Like in CYCLON, disconnected peers are eventually forgotten due to the timestamp property of the newscasting process. Newly joined peers' cache initialization is based on a regular news exchange with one or more peers who are already members of the overlay. In contrast, CYCLON uses \textit{c} random-walks exchanging exactly one cache entry with possibly different peers, which in turn helps to maintain the randomness properties; finally, while Newscast managed to achieve their goals of providing a robust, scalable, adaptive and lightweight solution for simultaneous membership management and aggregation CYCLON exhibits less network overhead\cite{cyclon}.

\section{Cloud-Assisted P2P Networks}

Many academic and commercial projects have been developed over the years where cloud, grid, and other centralized architectures are partly assisted by P2P networks. In these systems, peers are usually machines belonging to the clients of the service, and they help to decrease the load on the system, hence reducing its cost. This approach is especially prevalent in streaming and online gaming services, for example, Blizzard Activision uses their player base to help distribute installers and patches in a P2P fashion, and Digital Extremes uses, in their free-to-play game Warframe, P2P networks in particular, but not all, group missions where individuals are in instances of the game inaccessible by players outside the group. However, only recently, was the alternative explored, where P2P networks serve the vast majority of the service and are assisted by stable cloud services only to help guarantee some quality of service property of the system under critical situations. We present two of them.

\subsection{CLOUDCAST}
CLOUDCAST \cite{cloudcast} is a P2P-assisted cloud architecture that differs from other proposals because it does not offload the provided service to peers only when service availability is not affected. Furthermore, the architecture does not augment P2P systems with elastic computing nodes that perform particular tasks beyond the reach of the P2P network, such has bulk-synchronous content distribution. CLOUDCAST functionality depends solely on a passive storage service that transparently integrates into a gossip-based P2P system. The reasoning behind not using elastic computing instances is to eliminate fixed renting costs. Like that, the focus is on minimizing the costs associated with storage and bandwidth. The architecture uses two gossip protocols, one for membership management and another for information dissemination. Key results of the proposal include: accesses to the cloud service are bounded regardless of the number of clients in the system; the cloud service only distributes content when few peers are available; the overlay does not suffer from partitioning regardless of the number of peers in the system; the two illustrated use cases have an absurdly low yearly cost when compared to pure cloud approaches, and finally the authors recognize that the vast part of their bill is tied to membership management and not to actual storage costs, thus using strategies that adapt to the size of the network could yield even better savings.

The passive cloud storage service is a key-value store that can be accessed via GET and PUT operations, but it cannot under any circumstance initiate communications with the system peers. Two problems are identified, \textit{membership management} and \textit{information diffusion}. To solve the first problem, CLOUDCAST uses the CYCLON protocol to do peer-sampling. From other alternatives, CYCLON was chosen not for its particular inexpensiveness, but because the protocol maintains a random overlay topology that works even during high churn periods and even in the presence of catastrophic failures where up to 80\% of the peers fail. CYCLON properties also guarantee that references to the cloud server are not too many keeping usage cost under control. Finally, the cloud server can easily integrate in the protocol, in particular, CYCLON does not require the cloud server to initiate any communications, i.e., its active roles can be carried out by peers interacting with it; Finally, the cloud server can easily integrate into the protocol, in particular, CYCLON does not require the cloud server to initiate any communications, i.e., its active roles can be carried out by peers interacting with it; In this regard, the dummy process carried by the peers occasionally refreshes pointers to the cloud to avoid losing all references to it in the system. To deal with the case where all references cloud are lost due to the dynamic characteristics of the environment, e.g. churn and lost messages, all peers keep a logical timestamp of the last round they heard about the cloud server updating it whenever they successfully contact it or because one of their a \textit{neighbor} heard about it more recently then them, if after \textit{t} rounds peer has not updated this timestamp he will create a new cloud reference with probability $p\textsubscript{recovery}$.

The second problem is solved using two different implementations of gossip algorithms explained in depth in \cite{epidemic_algorithms}. They are a \textit{rumor-mongering, coin and blind} approach which favors fast news spreading and consists in having a peer \textit{i} periodically forwarding updates he learns about to some other peer \textit{j}, then deciding with probability $p\textsubscript{rumor}$ if he should, permanently, stop gossiping the update; And a \textit{anti-entropy, push-pull} approach which guarantees that news are eventually known by all participants of the system and consists in having each peer \textit{i} periodically contacting a random neighbor \textit{j} to exchange update they know about. All updates to the data are recorded in the cloud server through a PUT operation before becoming hot-topics, thus triggering rumor-mongering. Peers never contact the cloud server for rumor-mongering purposes. The anti-entropy approach can be piggybacked to the CYCLON peer-sampling step, thus decreasing message overhead; the authors, however, do not mention this optimization.

\subsection{CYCLON-CLOUDCAST: Adaptive Replica Management}

\begin{algorithm}[ht]
\caption{MakeDecision(N\textsubscript{e}), replicaOverlay}
\label{alg:makedecision}
\begin{algorithmic}
    \If {$N\textsubscript{e}\geq S$}
        \State $replicaOverlay.Neighbors.Remove(CloudReference)$\
        \If {$N\textsubscript{e} > R$}
            \State $p\textsubscript{yes} \gets \dfrac{N\textsubscript{e} - R}{N\textsubscript{e}}$
            \State $this.LeaveOverlay(replicaOverlay, p\textsubscript{yes})$
        \State $replicaOverlay.Protocol \gets cyclon$
        \EndIf
    \ElsIf {$C < N\textsubscript{e} < S$}
        \State $N\textsubscript{a} \gets [\dfrac{S(1 + k)}{N\textsubscript{e}} - 1]$
        \State $newNeighbors \gets GetNewNeighbors(this.UnderlyingOverlay, N\textsubscript{a})$
        \State $newNeighbors.Send(this.RandomView(replicaOverlay))$
        \State $replicaOverlay.Neighbors.Add(newNeighbors)$
        \State $replicaOverlay.Protocol \gets cloudcast$
    \ElsIf{$Ne \leq C$}
        \State $N\textsubscript{a} \gets [\dfrac{S(1 + k)}{N\textsubscript{e}} - 1]$
        \State $newNeighbors \gets GetNewNeighbors(this.UnderlyingOverlay, N\textsubscript{a})$
        \State $newNeighbors.Send(this.RandomView(replicaOverlay) \cup CloudReference)$
        \State $replicaOverlay.Neighbors.Add(newNeighbors \cup CloudReference)$
        \State $replicaOverlay.Protocol \gets cloudcast$
    \EndIf
\end{algorithmic}
\end{algorithm}

Following the suggestions of CLOUDCAST authors, H. Kavalionak \textit{et al.}, developed a decentralized algorithm \cite{marriage_of_convinience} that aims to do replica management in cloud-based, peer-assisted applications. Because the proposed work has no running title, we shall refer to it as ARM. The goal of the ARM is to maintain an appropriate replication level, despite churn, such that, at all times, at least one replica is available and ensuring that replicas survive failures. Furthermore, the proposal acknowledges that replicas should be synchronized, i.e., the system provides at least some consistency guarantees. In simple terms, this is perhaps the main difference between the proposed work and CLOUDCAST, which stores all updates in an always-on passive cloud server, helping with information diffusion and overlay connectivity and overall reliability of the system in extreme conditions. CLOUDCAST monetary cost reductions are primarily associated with limiting access to the cloud server. ARM effectively reduces those costs even further by performing timely switches between CLOUDCAST and regular CYCLON protocols, depending on the size of the network. To estimate the network size without centralization or broadcasting algorithms that would ultimately lead to either bottlenecks or overhead, ARM leverages a gossip-based aggregation method \cite{gossip-based_aggregation} that allows each peer to infer the approximate size of the network autonomously, at the end of each epoch. Estimation accuracy is dependent only on the size of the epoch. Likewise, the execution cost depends on the length of the epochs and the network churn rate, but not on the number of peers in the system, which means that ARM keeps the scale-free properties of the unstructured overlays that used in CLOUDCAST. The peers randomly organize themselves according to the CYCLON peer-sampling protocol, but depending on the results of the aggregation, the passive cloud-server used in CLOUDCAST is removed or kept in the overlay. In ARM, each data object to be replicated as its \textit{replica-overlay} on top of the regular unstructured overlay, which facilitates synchronization and cloud-usage decisions. The ARM algorithm is designed to consider four replica-overlay states, separated by three size thresholds: \textit{Critical (C)}, \textit{Sufficient (S)} and \textit{Redundant (S)}. \textit{C} depends on the chosen redundancy method, i.e., file-level \textit{vs.} block-level replication and is the sum of the minimum number of replicas required for data recovery plus the number of peers that will leave during the cloud backup replication phase. \textit{S} depends on the churn-rate and is the sum of \textit{C} plus the number of peers expected between two successive recovery phases. Finally, \textit{R} is a system parameter dependent also on the chosen redundancy method and is application requirements specific.  The aggregation method mentioned earlier is the foundation \textit{monitoring phase}, which consists of a fixed number of gossip and idle rounds, allowing a peer to infer the \textit{expected size (N\textsubscript{e})} of the replica-overlay and decide if he should add or remove the cloud or peers to the replica-overlay as well as the protocol he uses, based on Algorithm \ref{alg:makedecision}. After each monitoring phase, each peer calculates how long it should wait before executing the \textit{monitoring phase} again. That time depends mostly on \textit{N\textsubscript{e}}, e.g., if the state is below the critical threshold, the monitoring phase will initiate earlier than if it is above the redundant threshold, allowing the system to respond to overlay changes promptly and to keep the overlay size in a range that minimizes overhead but ensures that data does not become unavailable. The authors do not specify how much savings the system was capable of achieving using ARM; however, we can assume it is a substantial value; In Fig. \ref{fig:arm_results}, the plot on top displays the number of peers in the underlying network as a function of time, and the bottom plot shows the in-degree to the cloud server as a function of time, comparing CLOUDCAST and ARM (labeled as "our protocol").

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{images/armresults.PNG}
\caption{Cloud-Server in-degree, CLOUDCAST \textit{vs.} ARM \cite{marriage_of_convinience}}
\label{fig:arm_results}
\end{figure}

\newpage\section{Data Redundancy and File Survivability}\label{sec:dataredundancy}
In distributed storage systems, not losing files is of the utmost importance. Particularly, since peer availability in P2P networks is ever-changing and not guaranteed, the availability and even durability of a file may be endangered. A \textbf{\textit{durable file}} is one that, once put into the system, is not lost due to failures, whereas availability means that a user can access it promptly. For clarity, an available file is durable, but the other way is not necessarily correct. Distributed systems as a whole, usually ensure durability by employing file-level or block-level \textbf{\textit{replication}}; alternatively, some systems adopt \textbf{\textit{erasure coding}} techniques (\textit{coding} for short) or a mixture of both approaches. Replication has the advantage of having a more straightforward and less resource-intensive recovery model, on the other hand, coding has a better \textit{expansion factor}, i.e., can provide the same or even increased durability at a lower storage overhead. Examples already mentioned in this thesis proposal using replication include GFS and Hadoop, but there are many others; examples of modern systems using coding redundancy include Microsoft Azure and Facebook Analytics Hadoop Cluster \cite{lrc}. Because our proposal focuses on the survivability of files in P2P networks, using swarm guidance, where resources may be limited, we care only about coding in the survey that follows, as it allows more files to be kept by the system. We will briefly mention some of the raised concerns and inherent disadvantages associated with this technique and some of the suggested approaches to mitigate them over the two decades.

Two parameters specify some of the most widely used coding algorithms, the number of data symbols \textit{k} to be encoded, and the number of coded symbols \textit{n} to be produced, both assumed to be in a finite field $GF(2^w)$ and these are referred as \textit{(n, k) erasure-codes}. The fundamental idea, is that a file of size $S$ is divided into $k$ equally sized fragments and encode them into $n$ encoded fragments, allowing the original file to recovered from any set of encoded-fragment with count $k$. Systems that use these and other more sophisticated coding are evaluated under some of the following metrics, the \textbf{\textit{repair-bandwidth}}, i.e., the number of bits communicated in the network during normal and repair operations, the \textbf{\textit{disk I/O}} which is the number of bits read from disk during each repair, the \textbf{\textit{repair locality}} which is the number of peers that would be involved in the repair process; Since coding algorithms are slow processes CPU cycles, XOR operation counts and cache misses are metrics often researched as well \cite{fast_coding}, when the main interest is tied with load-balancing and cost savings. We take particular interest in the first metric because, in general, coding leads may lead to unreasonable bursts in bandwidth consumption when availability requirements, in very dynamic systems, are high \cite{coding-problems}, even for today's standards. Whenever a peer fails, all files he held need to be quickly recovered by some peer who needs to gather enough fragments from possibly many different peers. Such burst in consumption may lead to delays or even failure of the repair process due to physical link capacity between peers, who are involved in the repair process, or even throughput constraints enforced by each peer. When it comes to redundancy in a distributed storage systems, one fundamental question is deciding when to replicate or recover data; this is often called the replica-control problem.

There are two main approaches, reactive and proactive. Reactive approaches are the least complex, and consist of recovering data whenever a failure is detected; they suffer the most from bandwidth bursts, and mitigations to this problem include in using prediction models and thresholds to avoid unnecessary recoveries \cite{lifetime-reactive, efficient-replica-mng-reactive}. Proactive approaches seek to smooth bandwidth consumption by injecting new redundancy data into the network overtime at a fixed-rate \cite{proactive-rep, reliability-without-availability}. When it comes to coding techniques, the most widely known algorithms belong to the \textit{Maximum Distance Separable} (MDS) class, this class is likely to offer the best reliability-redundancy trade-off while consuming the least amount of storage space at the cost of increased repair-bandwidth, disk I/O and CPU time. Regardless of the chosen approach, research \cite{network-coding-for-dss} suggests that for coding to be competitive in distributed storage systems, one should seek to minimize these costs, while maximizing redundancy to only the amount necessary to ensure a specific service guarantee. They identify \textit{Minimum Bandwidth Regenerating} (MBS), and all codes in between these two ends of the spectrum as \textit{regenerating codes}, the differences between codes on the spectrum depend on the values such has \textit{n}, \textit{k} and on the implementation of the algorithms themselves. Shokrollahi \textit{et al.} introduces raptor codes \cite{raptor-codes}, which are a linear-time coding and decoding extension of LT codes \cite{lt-codes}. LT codes belong to the class of rateless codes, i.e., a potentially infinite sequence of encoded fragments can be generated from a given data source. Furthermore, the original data is recoverable from any subset of the encoded fragments of size equal to or slightly larger than original data size; These rateless code implementations are of particular importance because they are scalable with regards to the machine and network resources,  and fault-tolerant concerning dynamic and heterogeneous network environments. Finally, Zhou \textit{et al.} \cite{fast_coding} perform a study on state of the art techniques that improve coding efficiency through bitmatrix optimization, vectorization, and scheduling and prove that when used in conjunction much better levels of performance can be delivered, e.g., 552.27\% when compared to simple MDS implementations, in particular, the Reed-Solomon algorithm.

\newpage\section{Probabilistic Swarm Guidance}\label{sec:psg}
Markov chains describe a sequence of possible events in which the probability of each event depends only on the previous event states, often used to model real-world processes, such as queues of customers arriving at an airport and population growths. Markov processes are the basis for stochastic simulation methods known as Markov Chain Monte Carlo (MCMC), which allow one to perform random sampling over vast state spaces, i.e., computing expectations concerning complex, high dimensional probability distributions. The idea behind MCMC is creating a Markov Chain, which will, over time, tend to an equilibrium that is close to desired density distribution. In robotics and control areas Markov Chains and MCMC are used as the basis foundation for a process known as probabilistic swarm guidance, which gifts autonomous robots with the capability of generating their trajectories independently of each other, in decentralized fashion and without ahead-of-time position allocation, so that sets of robots, known as swarm, converges to the desired distribution shape. Swarm guidance has the advantage over other methods for this problem because robots do not necessarily need to communicate with each other to perform their tasks, even if they could gain from it; their overall distribution can be adapted dynamically to the environment, and any damage to their desired formation is eventually self-repaired. Demonstration of these properties can be found in recent research by B. Açikmeçe \textit{et al.} \cite{psg-mca, psg-caa, psg-onoff}, which inspired us to apply swarm guidance principles to P2P networks and file survivability. Boyd et al. \cite{fast_mixing_mc} argue that the most popular MCMC methods, Metropolis-Hastings and Maximum-Degree Chain, that allow the creation of transition matrices, that can then be followed independently by autonomous robots in order to practice swarm guidance behavior are not optimal with respect to mixing rates, i.e.,  these algorithms may not result in matrices that converge in small amount of steps towards the desired density distribution. They propose that the problem of obtaining such matrices can be formulated as a convex optimization problem and prove that, for most cases, the resulting matrices have higher mixing rates. It is reasonable to assume that, concerning our problem in P2P networks, this might open the possibility of imposing additional constraints on the creation of the transition matrix, e.g., bounding the probability of a peer sending a file to some other peer in his hive to increase bandwidth balancing.

%%%%%%%%%%%%%%%%%%%%
% END RELATED WORK %
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN SOLUTION PROPOSAL %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solution Proposal}\label{sec:proposal}
%%%%%%%%%%%%%%%%%%%%%%%%%
% END SOLUTION PROPOSAL %
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN EVALUATION METHODOLOGY %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation Methodology}\label{sec:methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END EVALUATION METHODOLOGY %
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN WORK SCHEDULE %
%%%%%%%%%%%%%%%%%%%%%%%
\section{Work Schedule}\label{sec:workschedule}
%%%%%%%%%%%%%%%%%%%%%
% END WORK SCHEDULE %
%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%
% BEGIN CONCLUSIONS %
%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%
% END CONCLUSIONS %
%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
% BEGIN REFERENCES %
%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs04}
\bibliography{bibliography}
%%%%%%%%%%%%%%%%%%
% END REFERENCES %
%%%%%%%%%%%%%%%%%%

\end{document}
%%%%%%%%%%%%%%%%
% END DOCUMENT %
%%%%%%%%%%%%%%%%
