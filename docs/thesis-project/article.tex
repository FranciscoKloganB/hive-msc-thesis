%% fsip2pnupsg.tex
%% V0.1
%% 2019/11/27
%% by Francisco Barros

%%%%%%%%%%%%%%%%%
% BEGIN IMPORTS %
%%%%%%%%%%%%%%%%%
\documentclass[runningheads]{llncs}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{optidef}
\usepackage{subfigure}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{todonotes}

\newcommand{\SubItem}[1]{{\setlength\itemindent{15pt} \item[-] #1}}
%%%%%%%%%%%%%%%
% END IMPORTS %
%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT %
%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%
% BEGIN HEADER %
%%%%%%%%%%%%%%%%
\title {Hives: File Survivability in P2P using Probabilistic Swarm Guidance
\thanks{This work has the support of the project MYRG2016-00097-FST of the University of Macau; by the Portuguese Fundação para a Ciência e a Tecnologia (FCT) through Institute for Systems and Robotics (ISR), under Laboratory for Robotics and Engineering Systems (LARSyS) project UID/EEA/50009/2019.}
}
\titlerunning{Hives}
\author{Francisco Barros, Daniel Silvestre \and Carlos Silvestre}
\authorrunning{F. Barros et al.}
\institute{Instituto Superior Técnico - Taguspark\newline Av. Prof. Doutor Cavaco Silva, 2744-016 Porto Salvo, Portugal
\email{fbarros@isr.ist.utl.pt, dsilvestre@isr.ist.utl.pt, csilvestre@umac.mo}\newline
\url{www.tecnico.ulisboa.pt}
}
\maketitle
%%%%%%%%%%%%%%
% END HEADER %
%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
% BEGIN ABSTRACT %
%%%%%%%%%%%%%%%%%%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in 150--250 words.
\begin{keywords}Agents-based Systems; Cooperative Control; Peer-to-Peer; Cloud Storage; Distributed control; File Availability; Swarm Guidance;\end{keywords}
\end{abstract}
%%%%%%%%%%%%%%%%
% END ABSTRACT %
%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN TABLE OF CONTENTS %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{tocdepth}{3}
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%
% END TABLE OF CONTENTS %
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% BEGIN INTRODUCTION %
%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Introduction}\label{sec:intro}

\subsection{Problem Description}
With the growth of the internet and the emergence of distributed systems, two large-scale computing paradigms have gained popularity due to their promise of virtually unlimited scalability. On the one hand, we have Peer-to-Peer (P2P) networking, which can be defined as a group of equally privileged peers, who contribute with a portion of their resources, to achieve common goals. These networks are popular among file sharing and streaming applications \cite{ssaroiu:msp2pfss}, due to their self-organized behavior, lack of centralization and, low-cost. On the other hand, Cloud platforms offer unmatched, on-demand, self-service, availability, and reliability, at a higher cost, and are currently trendy, with companies such as Amazon, Google, and Microsoft offering various ITaaS products to individuals and organizations alike. Cloud-based systems are centralized architectures, in which a large number of computers are clustered and managed by master entities, which possibly, become bottlenecks. Both paradigms can be used to deploy distributed file-storage or file-backup applications; despite the fact that P2P implementations are cheaper for both companies and their clients, these have a hard-time achieving 99.9\% availability seen Cloud-based implementations, furthermore, depending on the P2P application there may also exist the risk of permanent file loss, making them somewhat unappealing for clients who want to store or backup their files remotely.

\subsection{Our approach}
The focus of this thesis is to create a structured P2P overlay to be used on a distributed backup system, where clients upload their files to remote peers, who are selling their storage space, without persistence guarantees. Probabilistic Swarm Guidance (PSG) and Markov Chain theories \cite{markovchain_approach_to_pga} will be used, along with block-based file replication using erasure code algorithms and a reputation system, to hopefully create a network that is able to keep a file available at any given time, regardless of presence of attackers, network partitions and amount of churn, i.e., no matter how many peers leave/join an overlay.

\subsection
{Frequent approach}There has been much research on the topic of P2P overlays. There are two significant categories of overlays, Structured and Unstructured. In the former, the overlay has rigid rules regarding peer placement; they favor lookup speed but have higher maintenance overhead under churn, i.e., the process of peers leaving or joining the overlay. Some popular P2P Structured overlays include Chord \cite{chord}, Tapestry \cite{tapestry}, Kademlia \cite{kademlia}, and P-Grid \cite{pgrid}. In the latter, peer placement is random, which provides the overlay network with a higher degree of resilience and robustness in the advent of failures. Unstructured overlays disseminate information in a gossip-like fashion; thus, lookup operations are slower, resource management is less efficient, but overlay maintenance is often straightforward. Popular  Unstructured P2P overlays include FreeNet \cite{freenet}, Gnutella \cite{gnutella} and BitTorrent \cite{bittorrent}. Research on multi-layer overlays has not yielded satisfactory results, and implementation is difficult \cite{sotart}, but research on bio-inspired overlays did have promising results, some of these include Self-Chord \cite{selfchord}. Finally, there have also been some attempts of transparently integrating Cloud Systems into P2P networks, e.g., CLOUDCAST \cite{cloudcast}, and vice-versa, e.g., Spotify \cite{spotify} and Wuala \cite{wuala}. From all the mentioned research, perhaps the one that borrows most similarity with our proposal is CLOUDCAST; however, neither it nor any of the remaining literature, try to create a structured overlay composed of completely autonomous peers, who work together to keep the file alive without enforced persistence based on PSG.

\subsection{Proposal Goals}\label{subsec:intro}
\begin{itemize}
    \item Implement a Tracker server with light-weight responsibilities:
        \SubItem{Act as peer discovery service for clients;}
        \SubItem{Act as an indexer for clusters of peers (Hives);}
        \SubItem{Track peer reputations;}
    \item Implement an erasure-code algorithm to ensure file persists in the Hive;
    \item Implement a trust system where:
        \SubItem Each peer gives feedback to the Tracker about his neighbors;
        \SubItem Tracker synthesizes the feedback and presents it to the client;
    \item Calculate the ideal steady-state distribution of file blocks, for a Hive, when:
        \SubItem{No trust history on a new peer is available;}
        \SubItem{Peer availability, reliability and other trust factors are known;}
    \item Hives converge to ideal block distribution in bounded-times using PSG;
    \item Efficiently update the PSG's Markov Chain as it changes over time;
    \item Balance the in-degree and out-degree of each peer such that:
        \SubItem{Message exchange overhead is minimal;}
        \SubItem{Any changes to the probability of file loss are negligible;}
        \SubItem{Hive availability, reliability, and, confidentiality are not compromised;}
        \SubItem{Hive is resilient to massive node failures and resistant to churn;}
    \item Extra miles may include:
        \SubItem{Peers can reconstruct Hives without Tracker intervention;}
        \SubItem{Peers trust their neighbors independently of Tracker;}
        \SubItem{Hives can leverage dedicated Cloud-Storage if they become unreliable;}
\end{itemize}
%%%%%%%%%%%%%%%%%%%%
% END INTRODUCTION %
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% BEGIN RELATED WORK %
%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Related Work}\label{sec:relatedwork}
This section as the following structure: In the first subsection, we will study two distributed file systems used in cloud architectures, in the following two subsections we look at four P2P overlays. We will then mention one cloud-assisted P2P protocol that we consider relevant for being designed to the same use-case that led us to make this thesis proposal. Then we will take a look at two well-known research regarding trust in P2P networks, and finally, we will study erasure code algorithms for file-block replication.
\subsection{Distributed File Systems}
\textbf{Google File System} (GFS) is designed to provide efficient and reliable access to data using clusters of commodity machines located in geographically spread data-centers. In particular, GFS aims to provide high data throughput, low latency, and reliability in the face of individual server failures. Because GFS runs over commodity hardware, the proposed model considers machine and network faults as being the norm rather than the exception \cite{gfs}.  This assumption is typical in modern distributed systems of any kind. GFS is the basis for an open-source project, which has seen continuous improvements over the years, called Hadoop Distributed File System (HDFS) \cite{hadoop}; we will not discuss it here because it holds many similarities to GFS with key differences being the approach to security management. One advantage of GFS over other architectures is that it leverages a distributed file system that tailors to the designers observation that, once written, files on google provided services, are only read, often sequentially, with random writes being practically non-existent, shifting optimization focus from latency to throughput, from random writes to appends when it comes to atomicity and no caching of data blocks \cite{gfs}. In the GFS architecture cluster consists of a single master and multiple chunk-servers. The Master maintains a mapping from files to chunks, the current location of chunks, and is responsible for failure detection over the chunk-servers as well as chunk-migration between them. Clients communicate, for reads and writes, with the Master for metadata operations and directly with the chunk-servers for data-bearing operations. Neither the client nor the chunk-servers cache file data because most of google's client applications stream through huge files. A read operation involves obtaining chunk handles and locations from the Master and then retrieving the chunks from one of the many available chunk-severs. For writes, the client splits the file into 64MB chunks and asks the Master to assign them an identifier, and then the client uploads his chunks to the chunk-servers indicated by the Master.

GFS has proven its qualities in supporting large-scale data processing. However, small files have shown \cite{gfs} to lead to overloads on chunk-servers holding them because chunks have a fixed large value size of 64MB. The reasoning for GFS to use such substantial value is to reduce network overload, reduce the number of accesses to the Master, and reduce memory used at the Master due to mappings. Concerning our proposal, Hives, since convergence to a desired steady-state using Markov Chains theory requires a large number of chunks to exist in a P2P Hive, we might have to do file-level replication rather than the desired block-level when client files are too small.\newline

\textbf{Ceph} is an open-source, storage system that, like GFS, runs over commodity hardware. Ceph aims to deliver reliable, autonomous, distributed object, block, and file storage that is self-healing, self-managing, and has no single point of failure. The base architectural component of Ceph is the RADOS object store, and all content a client reads and writes to the system are represented as objects, irrespective of the objects' original data type. RADOS is for all purposes a Ceph cluster, and its storage space originates from the disks of multiple machines. Ceph architecture is slightly more complicated than that of GFS, so we will summarize the components that are of interest to us. Inside a cluster, we find three types of machines, storage devices, abstracted with a layer called Object Software Daemon (OSD), Monitors, and Metadata Servers. A small count of Monitors maintain cluster membership and state using PAXOS \cite{paxos} to make consensual decisions. Metadata Servers only required sharing the distributed file system, are used for POSIX semantics, e.g., directory hierarchy and file metadata such as owner and last accessed timestamps. Monitors can detect if metadata servers are down and launch new ones as required. Finally, OSDs store clients' data, and there are anywhere between dozens to thousands of these in a Ceph cluster.

The innovation Ceph brings is that the OSDs within a cluster are organized in a P2P fashion and are capable of autonomously carrying out replication and recovery tasks. When one OSD suspects a peer as died or joined the cluster, it consults the Monitors to obtain an updated cluster map and possibly offloads some of his data to some other living OSD. The way an OSD knows based on the cluster map if he needs to send data to another OSD is the same process a client goes through to discover to which OSD he should write or read his data. In all cases, a client or OSD process runs the Ceph.CRUSH(placement groups, cluster state, rule set) to obtain the Target OSDs of the named object. The last two parameters are obtained directly from the monitors. The first parameter is the result of the operation: hash(object name) mod (size of object pool). CRUSH \cite{crush} is a light-weight pseudo-random placement algorithm, that provides deterministic object quantization without lookups to central directories. Because it runs independently on clients and peers, it avoids the need to have the monitor constantly updating cluster to object mappings.

Ceph is proof that cloud systems can leverage P2P behavior in order to dynamically adjust replication in a decentralized fashion while meeting high dependability standards. \cite{ceph, ceph_benchmarks}

\subsection{Structured Overlays}
\textbf{Kademlia} is structured overlay, yet it is fully decentralized, designed for key-value storage systems, which, according to the authors, combines provable consistency, performance, latency-minimizing routing, and symmetric topology \cite{kademlia}. Kademlia peer-identifiers and data-keys are built as in Chord\cite{chord}, using consistent hashing. Inserts and lookups on the overlay are unidirectional, according to the distance between two identifiers, calculated using XOR. The unidirectionality allows caching to be done along lookup paths, alleviating hot spots. Each peer in the overlay maintains a binary tree, routing table, whose leaves are lists of peers, called k-buckets, at distances $2^{x}$ to $2^{x+1}$, $x \in\hspace{1mm}\mathclose[0,160\mathclose]$ from itself, sorted according to a least recently seen policy. As a peer receives messages if the sender is a new acquaintance, that sender is placed on the proper k-bucket if there is enough space in it; otherwise, it will replace the least seen peer only if that peer fails to respond to a ping. By preferring old acquaintances, Kademlia seeks to enhance the stability of the overlay based on the observation that the longer a peer remains connected to the network, the higher the probability that it will remain connected at least another hour\cite{ssaroiu:msp2pfss}. This policy also prevents some denial of service attacks, where an enemy floods the network with new peers. The XOR metric gives Kademlia an advantage over other DHT-based implementations by making routing tables less rigid and by allowing peers in the network to learn useful routing information from discovery and lookup messages. Because of the existence of k-buckets, routing can also be done around failed nodes, providing some resistance to churn and partitioning.

The standard Kademlia implementation, however, does not implement an effective mechanism to keep dated entries out of a peer's k-buckets, meaning that the longer a peer stays offline, the worse is k-buckets are. Dated entries removed when the peer contacts a peer and obtains no response, meaning that insert and lookup operations may take longer than they could. The take-away from Kademlia protocol is that both the liveness and the freshness of an overlay can be improved by passively or actively making peers monitor each other.\newline

\textbf{Self-Chord} is a bio-inspired, structured overlay, specifically designed to be used in cloud computing or grid computing architectures. Self-chord foundations are that of the standard Chord, both in terms of identifiers and the way peers organize themselves on the overlay, i.e., in ring-like shapes. However, it has a few benefits, which are the result of having mobile agents, called ants, independently roaming the overlay network, but collaboratively re-arranging resource identifiers so that similar data items are clustered around neighboring peers with relatively proportional workloads, meaning that data item placement is not based solely on the numerical-space shared between them and possible placement peers \cite{sotart, selfchord}. Hence advantages of Self-Chord over the standard implementation include lack of need to assign data to well-specified peers since keys are independently defined, leading to better behavior under high churn conditions. Less overlay maintenance overhead since when a peer joins or leaves the network, ants will autonomously and eventually, distribute data items appropriately; Lastly, since load-balancing is continuously being carried out by ants, the scalability and robustness of the system increases since individual peer failures are less likely to lead to massive file loss. According to the authors, the behavior of the ants ensures that even under highly dynamic, unfavorable conditions, rearrangement, and discovery of data items takes only logarithmic time.

Bio-inspired systems like the one above, diminish or eliminate inherent disadvantages typically associated with structured or unstructured overlays, such as the ones mentioned in the introduction of this thesis proposal. In these, agents with simple behavior often accomplish complex behavior that would otherwise require time or space consuming algorithms, sometimes with high code complexity. In our proposal, we, too, seek to create a system that, just like Self-Chord, has self-healing and self-organizing behaviors.

\subsection{Unstructured Overlays}
\textbf{Freenet} laalalalaa\newline

\textbf{Gia} lalala\newline
\subsection{Cloud-Assisted P2P Protocol - CLOUDCAST}
\subsection{File Block Replication}
\subsection{Trust in Peer-to-Peer Networks}
%%%%%%%%%%%%%%%%%%%%
% END RELATED WORK %
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN SOLUTION PROPOSAL %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Solution Proposal}\label{sec:proposal}
%%%%%%%%%%%%%%%%%%%%%%%%%
% END SOLUTION PROPOSAL %
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN EVALUATION METHODOLOGY %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation Methodology}\label{sec:methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END EVALUATION METHODOLOGY %
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN WORK SCHEDULE %
%%%%%%%%%%%%%%%%%%%%%%%
\section{Work Schedule}\label{sec:workschedule}
%%%%%%%%%%%%%%%%%%%%%
% END WORK SCHEDULE %
%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%
% BEGIN CONCLUSIONS %
%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%
% END CONCLUSIONS %
%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
% BEGIN REFERENCES %
%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs04}
\bibliography{bibliography}
%%%%%%%%%%%%%%%%%%
% END REFERENCES %
%%%%%%%%%%%%%%%%%%

\end{document}
%%%%%%%%%%%%%%%%
% END DOCUMENT %
%%%%%%%%%%%%%%%%
