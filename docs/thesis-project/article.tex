%% fsip2pnupsg.tex
%% V0.1
%% 2019/11/27
%% by Francisco Barros

%%%%%%%%%%%%%%%%%
% BEGIN IMPORTS %
%%%%%%%%%%%%%%%%%
\RequirePackage{amsmath}
\documentclass[runningheads]{llncs}
\usepackage{indentfirst}
\usepackage{cite}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{optidef}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{todonotes}

\newcommand{\SubItem}[1]{{\setlength\itemindent{15pt} \item[-] #1}}
%%%%%%%%%%%%%%%
% END IMPORTS %
%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT %
%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%
% BEGIN HEADER %
%%%%%%%%%%%%%%%%
\title {Hives: File Survivability in P2P using Probabilistic Swarm Guidance
\thanks{This work has the support of the project MYRG2016-00097-FST of the University of Macau; by the Portuguese Fundação para a Ciência e a Tecnologia (FCT) through Institute for Systems and Robotics (ISR), under Laboratory for Robotics and Engineering Systems (LARSyS) project UID/EEA/50009/2019.}
}
\titlerunning{Hives}
\author{Francisco Barros, Daniel Silvestre \and Carlos Silvestre}
\authorrunning{F. Barros et al.}
\institute{Instituto Superior Técnico - Taguspark\newline Av. Prof. Doutor Cavaco Silva, 2744-016 Porto Salvo, Portugal
\email{fbarros@isr.ist.utl.pt, dsilvestre@isr.ist.utl.pt, csilvestre@umac.mo}\newline
\url{www.tecnico.ulisboa.pt}
}
\maketitle
%%%%%%%%%%%%%%
% END HEADER %
%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
% BEGIN ABSTRACT %
%%%%%%%%%%%%%%%%%%
\begin{abstract}
Peer-to-Peer (P2P) systems have emerged as a potential choice to build large scale distributed storage systems, at a fraction of the cost of the alternative cloud approaches, which have better quality of service guarantees. We survey  distributed file systems, P2P overlays, cloud-assisted P2P protocols, and erasure-code algorithms. We then draft a solution for the problem of creating a P2P overlay to be used on a cloud-assisted P2P backup system that uses probabilistic swarm guidance and aims to ensure file durability. We anticipate that our solution will have high network overhead, in order to provide good self-healing properties to the files stored in the system.

\begin{keywords}Agents-based Systems; Cooperative Control; Peer-to-Peer; Cloud Storage; Distributed control; File Availability; Swarm Guidance;\end{keywords}
\end{abstract}
%%%%%%%%%%%%%%%%
% END ABSTRACT %
%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN TABLE OF CONTENTS %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{tocdepth}{3}
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%
% END TABLE OF CONTENTS %
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% BEGIN INTRODUCTION %
%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Introduction}\label{sec:intro}

\subsection{Problem Description}
With the growth of the internet and the emergence of distributed systems, two large-scale computing paradigms have gained popularity due to their promise of virtually unlimited scalability. On the one hand, we have Peer-to-Peer (P2P) networking, which can be defined as a group of equally privileged peers, who contribute with a portion of their resources, to achieve common goals. These networks are popular among file sharing and streaming applications due to their self-organized behavior, lack of centralization and, low-cost. On the other hand, Cloud platforms offer unmatched, on-demand, self-service, availability, and reliability, at a higher cost, and are currently trendy, with companies such as Amazon, Google, and Microsoft offering various ITaaS products to individuals and organizations alike. Cloud-based systems are centralized architectures, in which a large number of computers are clustered and managed by master entities, which possibly, become bottlenecks. Both paradigms can be used to deploy distributed file-storage or file-backup applications; although P2P implementations are cheaper for both companies and their clients, these have a hard-time achieving availability levels seen in Cloud implementations. P2P systems also have a higher inherent risk of permanent file loss, making them somewhat unappealing for clients.

\subsection{Frequent approach}
P2P research often focuses on optimizing the overlay topology to provide load-balancing, read/write efficiency, and partition avoidance, implicitly assuming that reliable overlays provide reliable systems, in the case of distributed storage systems, replication or erasure coding is usually written on top of the P2P protocol to ensure durability. The most common approach for distributed storage systems, however, is done on the cloud using highly available machines, with expensive file or block-level replication, continuously monitored by failure detection entities to ensure the replication level is always above some threshold; While effective, the latest architecture is very costly and inefficient. Finally, there have also been some attempts of transparently integrating Cloud Systems into P2P networks, e.g., Cloudcast \cite{cloudcast}, and vice-versa, e.g., Spotify\cite{spotify} and Wuala\cite{wuala}. To the best of our knowledge, there is no system using PSG as a way of simultaneously deciding data routes, failure-detection, load-balancing, and, thus, file survivability in distributed storage systems.

\subsection{Our approach}
The focus of this thesis is to create a distributed backup system. Probabilistic Swarm Guidance (PSG) and Markov Chain theories will be used, along with erasure coding. We hope not to lose the files regardless of the amount of churn\footnote{Property of P2P systems, which regards peers joining or leaving a P2P network} or other experienced faults. Furthermore, to boost the availability and reliability files, cloud servers might be sporadically used to assist the P2P networks when these are facing critical conditions. The main difference between our approach and the frequent ones is that we use a layered solution, where a typical client-server approach facilitates lookup operations and provides system-wide but file durability, load-balancing, and failure detection is the responsibility of multiple P2P overlays specifically tailored for backing up files.

\subsection{Proposal Goals}\label{subsec:intro}
This proposal seeks to address the problem of file survivability in P2P networks; our primary goal is to design and evaluate a P2P overlay that uses probabilistic swarm guidance and erasure-coding as a mean to provide file survivability. Our secondary goal is to integrate instances of this overlay on a broader centralized system that provides fast look-up operations and increases availability and reliability by making connections to the cloud when needed. Steps include:

\begin{itemize}
    \item Identify a steady-state, $Q$, providing optimal robustness and load-balancing
    \item Identify a transition matrix, $M$, that allows peers to achieve $Q$ collectively
    \item Optimize $M$ with respect to peer connectivity degree
    \item Optimize updates to $M$ as overlay topology changes over time
    \item Implement a replication model that exploits the properties of the $M$
    \item Evaluate solution performance in a Simulation or on in real life scenario
\end{itemize}
%%%%%%%%%%%%%%%%%%%%
% END INTRODUCTION %
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
% BEGIN RELATED WORK %
%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Distributed File Systems}\label{sec:distributed-file-systems}
Distributed file system (DFS) enables individuals and organizations to store, access, and secure data on possibly remote locations, as they would their local machines. It is common practice to do file-level or block-level, but it is rare to use erasure-coding. The choice depends on the solution requirements and the broadness of the system. For example, block replication allows parallel execution of applications using MapReduce algorithms, and because of that, Google File System\cite{gfs} (GFS) uses block-level replication. We present two different implementations running on commodity hardware and focus on their architecture, advantages, or disadvantages. Before that, it is essential to understand some desired characteristics of a DFS. In general, distributed systems of any kind should be \textbf{\textit{dependable}}, meaning that they should be able to avoid service failures that are more frequent and more severe than is acceptable. To be dependable is to be ready to provide the correct service (\textbf{\textit{availability}}), in a continuous manner (\textbf{\textit{reliability}}), with absence of catastrophic consequences to clients (\textbf{\textit{safety}}), e.g., losing a file, with no improper system alterations (\textbf{\textit{integrity}}) and, finally, to be able to undergo modifications and repairs at any time (\textbf{\textit{maintainability}}). Some of the features that contribute to the above characteristics are the following: \textbf{\textit{load-Balancing}} is the process of balancing communication, processing, and storage overhead among all nodes of the system. It helps to avoid bottlenecks, increasing operational performance, and helps to prevent catastrophic failures. Prevention of failures is also achieved through \textbf{\textit{fault tolerance}}, which enables a system to continue operating, possibly at a reduced level,  rather  than failing completely, when some part of it fails, to this end, some sort of \textbf{\textit{machine redundancy}} and \textbf{\textit{data redundancy}} may be important. As the amount of data passing in the network, being processed or stored in the system grows, or the number of clients using it increases, the capacity of a system to maintain its service without, or with little degradation, is called \textbf{\textit{scalability}}, it depends on the implementation of the system, and as a rule of thumb, centralized solutions are less scalable.

\subsection{Google File System}
GFS wants to provide efficient and reliable access to data using clusters located in geographically spread data-centers. In particular, GFS aims to provide high data throughput, low latency, and reliability in the face of individual server failures. Because GFS model considers machine and network faults as being the norm, as is, nowadays typical. GFS serves as inspiration for a massively adopted open-source project called Hadoop Distributed File System\cite{hadoop}; we will not discuss it here because it holds many similarities to GFS with key differences being the approach to security management. Based on the author's observation that once written, files on google services, are only read, often sequentially, with random writes being practically non-existent, GFS chooses to optimize throughput over latency, appends over random writes and deliberately skips data block caching. GFS clusters consists of a single master and multiple chunk-servers. The Master maintains a mapping from files to chunks, the current location of chunks, and is responsible for failure detection over the chunk-servers as well as chunk-migration between them. Clients communicate, for reads and writes, with the Master for metadata operations and directly with the chunk-servers for data-bearing operations. A read operation involves obtaining chunk handles and locations from the Master and then retrieving the chunks from one of the many available chunk-severs. For writes, the client splits the file into 64MB chunks and asks the Master to assign them an identifier, and then the client uploads his chunks to the chunk-servers indicated by the Master. GFS has proven its qualities in supporting large-scale data processing. However, small files have shown to lead to overloads on chunk-servers holding them because chunks have a fixed large value size of 64MB. The reasoning such substantial size value is to reduce network overhead, reduce the number of accesses to the Master, and reduce memory used at the Master due to mappings. Concerning our proposal, since convergence to a desired steady-state is likely better achieved when more parts exist on the P2P network, we might consider using file replication rather than erasure-coding when files are small.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/gfs_paper.png}
\caption{GFS Architecture}
\label{fig:gfs_arch}
\end{figure}

\subsection{Ceph File System}

\begin{figure}[t]
\centering
\includegraphics[width=0.6\textwidth]{images/cephfs.png}
\caption{CFS Architecture}
\label{fig:cfs_arch}
\end{figure}

Ceph \cite{ceph} is an open-source system that aims to deliver reliable, autonomous, distributed object, block, and file storage that is self-healing, self-managing, and has no single point of failure. The base architectural component of Ceph is the RADOS object store. All clients' reads and writes to the system are represented as objects, irrespective of the objects' original data types. RADOS is a single logical entity composed of many storage clusters. Ceph architecture is sophisticated, so, we summarize the main components. Inside a storage cluster, we find four types of logical machines, storage devices, abstracted with a layer called \textit{object software daemon} (OSD), \textit{monitors}, \textit{managers}, and \textit{metadata servers}. Metadata servers contain information used for POSIX semantics, e.g., file owner or last accessed timestamps. A small number of monitors maintain a master copy of the storage cluster map with its current state. Monitors use Paxos to decide on the state of the cluster. Managers maintain detailed information about placement groups, process and host metadata. Finally, OSD store data on behalf of clients. Additionally, OSD utilize their nodes' resources to perform replication, erasure coding, load-balancing, recovery, monitoring, and reporting functions. Each Storage Cluster as anywhere from dozens to thousands of these OSDs. Ceph's innovation is that OSD within the same \textit{placement group}, within a cluster, are organized in a P2P fashion and are capable of autonomously carrying out recovery tasks. Placement groups are fragments of a logical \textit{pool}, and in turn, pools are logical, dynamic partitions that define among other things, how much and which type of replication is done to objects that belong to it. When one OSD in a placement group suspects another as left or joined the cluster, it consults a Monitor to obtain an up to date cluster map. After running the CRUSH algorithm\cite{crush} over the cluster map, it is possible that the OSD will offload some of its content to another OSD. Clients also use CRUSH to perform reads and writes to the system. To perform a write operation, for example, a client connects to a pool in the storage cluster and runs CRUSH with a cluster map retrieved from a Monitor. The result of the operation gives the client information on the placement group of the data he wishes to upload as well as the OSD he should contact to complete his request. When the target OSD receives the request, it takes the object identifier, pool name, and the cluster map and uses CRUSH, to discover the number of replicas that it should store and to which secondary OSDs he must replicate the write. The operation is completed when the primary OSD, receives acknowledgments from the remaining secondary OSD and acknowledges back to the client. Because operations runs independently on clients and OSDs, there is no need to have the monitor constantly updating cluster to object mappings. Ceph is proof that DFS can reliably be implemented using P2P behavior. Ceph OSDs perform a lot of self-monitoring and management operations and in a decentralized fashion while being highly dependable\cite{ceph_benchmarks}; we hope to be able to replicate these achievements in our approach.

\section{P2P Overlays}\label{sec:p2p-overlays}

\begin{figure}[t]
    \centering
    \subfigure{\label{fig:iplayer}\includegraphics[width=0.35\textwidth, clip=true]{images/ip-layer.png}}
    \hspace{0.2\textwidth}
    \subfigure{\label{fig:overlay}\includegraphics[width=0.35\textwidth, clip=true]{images/overlay-layer.png}}
    \caption{P2P Network Diagrams: IP layer \textit{vs.} One possible overlay abstraction}
    \label{fig:network_diagrams}
\end{figure}

In large-scale P2P systems, it is unfeasible and even undesirable for each peer in the network to know and explicitly interact with every other node connecting it to other peers. In this regard, a P2P overlay is a logical network that abstracts the physical network underneath it, typically IP-based, and where, single-hop edges represent links between pairs of peers, as depicted in Fig. \ref{fig:network_diagrams}. We define some properties and technical jargon related to overlays and graphs that help us understand the research we survey in the next subsections. An overlay is said to be \textbf{\textit{connected}} if there is a path that allows any pair of peers $(i, j)$ to communicate with each other, i.e., there is no partitioning and no peer is isolated in the system. The \textbf{\textit{degree}} of a peer is the total number of connections established between a peer \textit{i} and other peers. We can divide the degree into \textit{in-degree} and \textit{out-degree}, i.e., the number of channels a peer \textit{i} has, through which it can only receive data or send data, respectively. When all peers have the same degree, the overlay is said to be \textbf{\textit{regular}} or \textbf{\textit{symmetric}}. We can use the \textbf{\textit{degree distribution}} to evaluate the robustness of an overlay since through it; it is possible to detect \textit{weakly connected peers} and \textit{massively connected hubs}, i.e., peers with very high degree. Hubs may provide the overlay with resistance against random damage and possibly speed-up lookup operations in unstructured overlays that use biased searching; however, hub failures will have significant impacts on the connectivity of the overlay, severely damaging performance or even halting the system \cite{webdragons, controlling_the_hubs}. Peer degrees are also indicators of fair work distribution among peers. \textbf{\textit{Network diameter}} is the number of links that compose the longest of all the shortest paths between any two peers $(i, j)$. Knowing the length of all shortest paths between all possible pairs of peers allows us to extract the \textbf{\textit{average path length}} and derive how messages, on average, will be relayed on the overlay to contact a stranger. Still regarding distances, the path stretch is the ratio between the number of links a message has to traverse, between any two peers, in the underlying network and the overlay network. \textbf{\textit{Clustering}} directly affects connectivity and network diameter; we can measure it using the \textbf{\textit{clustering coefficient}}. This coefficient represents the percentage of neighbors a peer \textit{i} has, who are also neighbors among themselves. Like peers with high degrees, high clustering coefficients indicate the increased chance of partitioning in the system in the event of massive peer failures and \textit{churn}. Furthermore, it may indicate, in gossip-based protocols, bandwidth overhead due to redundant communications in the cluster, but has the advantage of making lookups faster even when the network is experiencing heavy load, e.g., flash crowd scenarios. Another two terms that often appear in literature are the \textbf{\textit{small-world}} and the \textbf{\textit{power-law}} properties. The small-world property is an umbrella term for a system with a high clustering coefficient and low average path length, yet most peers are not direct neighbors of each other. The power-law property means that there is non-uniform distribution influence in the overlay, e.g., some peers have more responsibilities or perks because they have higher capacities than the rest.

\subsection{Structured Overlays}
Structured overlay ensures peer placement follows rigid rules in order to speed up lookup operations, commonly using Distributed Hash Tables or Tree implementations. However, for this reason, these overlays also have higher maintenance costs due to churn, making them less ideal for highly dynamic environments.

\subsubsection{Kademlia} \cite{kademlia} is fully decentralized and designed for key-value storage systems, which, according to the authors, combines provable consistency, performance, latency-minimizing routing, and symmetric topology. Kademlia peer-identifiers and data-keys are built as in Chord\cite{chord}, using consistent hashing. Inserts and lookups on the overlay are unidirectional, according to the distance between two identifiers, calculated using XOR. The unidirectionality allows caching to be done along lookup paths, alleviating hot spots. Each peer in the overlay maintains a binary tree, routing table, whose leaves are lists of peers, called k-buckets, at distances $2^{x}$ to $2^{x+1}$, $x \in\hspace{1mm}\mathclose[0,160\mathclose]$ from itself, sorted according to a least recently seen policy. As a peer receives messages if the sender is a new acquaintance, that sender is placed on the proper k-bucket if there is enough space in it; otherwise, it will replace the least seen peer only if that peer fails to respond to a ping. By preferring old acquaintances, Kademlia seeks to enhance the stability of the overlay based on the observation that the longer a peer remains connected to the network, the higher the probability that it will remain connected at least another hour\cite{ssaroiu:msp2pfss}. This policy also prevents some denial of service attacks, where an enemy floods the network with new peers. The XOR metric gives Kademlia an advantage over other DHT-based implementations by making routing tables less rigid and by allowing peers in the network to learn useful routing information from discovery and lookup messages. Because of the existence of k-buckets, routing can also be done around failed nodes, providing some resistance to churn and partitioning.

The standard Kademlia implementation, however, does not implement an effective mechanism to keep dated entries out of a peer's k-buckets, meaning that the longer a peer stays offline, the worse is k-buckets are. Dated entries removed when the peer contacts a peer and obtains no response, meaning that insert and lookup operations may take longer than they could. The take-away from Kademlia protocol is that both the liveness and the freshness of an overlay can be improved by passively or actively making peers monitor each other.\newline

\subsubsection{Self-Chord} \cite{selfchord} is a bio-inspired, structured overlay, specifically designed to be used in cloud computing or grid computing architectures. Self-chord foundations are that of the standard Chord, both in terms of identifiers and the way peers organize themselves on the overlay, i.e., in ring-like shapes. However, it has a few benefits, which are the result of having mobile agents, called ants, independently roaming the overlay network, but collaboratively re-arranging resource identifiers so that similar data items are clustered around neighboring peers with relatively proportional workloads, meaning that data item placement is not based solely on the numerical-space shared between them and possible placement peers. Hence advantages of Self-Chord over the standard implementation include lack of need to assign data to well-specified peers since keys are independently defined, leading to better behavior under high churn conditions. Less overlay maintenance overhead since when a peer joins or leaves the network, ants will autonomously and eventually, distribute data items appropriately; Lastly, since load-balancing is continuously being carried out by ants, the scalability and robustness of the system increases since individual peer failures are less likely to lead to massive file loss. According to the authors, the behavior of the ants ensures that even under highly dynamic, unfavorable conditions, rearrangement, and discovery of data items takes only logarithmic time.

Bio-inspired systems like the one above, diminish or eliminate inherent disadvantages typically associated with structured or unstructured overlays, such as the ones mentioned in the introduction of this thesis proposal. In these, agents with simple behavior often accomplish complex behavior that would otherwise require time or space consuming algorithms, sometimes with high code complexity. In our proposal, we, too, seek to create a system that, just like Self-Chord, has self-healing and self-organizing behaviors.

\subsection{Unstructured Overlays}
Unstructured overlays usually disseminate information in a gossip-like fashion; thus, lookup operations are slower than structured overlays, resource management is less efficient, but overlay maintenance is often straightforward. Because peer placement is arbitrary, they tend to provide better scalability and a higher degree of resilience in the advent of failures.

\subsubsection{Gia} \cite{gia} is a decentralized overlay protocol whose purpose is to overcome the drawbacks of Gnutella. Gnutella is the first-ever decentralized, unstructured P2P protocol. Gnutella's main issues are lack of scalability, as resource discovery bases itself on simple message flooding mechanisms and how easily some peers become overloaded when faced with high rates of aggregate queries, leading to system degradation. Gia proposes the use of super-nodes, which are now also part of the Gnutella protocol. The super-nodes receive and route queries to peers holding data, but these super-nodes and the construction of the topology around them is dynamic and adaptive. Gia recognizes the heterogeneity and resource constraints of the peers and also uses random walks to alter the topology adaptively as well as lookup operations. The results show that Gia is three to five orders of magnitude better than that of Gnutella. Random-walks by themselves, while minimizing the network overhead, are less likely to find appropriate responses to the performed lookup, unless they are biased to high-degree peers, but this can make peer overloading more frequent. Thus, to tackle both problems, Gia implements a topology adaptation that puts most peers within reach of high capacity peers, while simultaneously ensuring that they can handle the likely incoming requests. Lastly, all peers keep pointers to data items kept by their immediate one-hop neighbors.

The algorithm Gia uses to ensure high-capacity-peers are also high-degree ones depends on a satisfaction function that returns a value in $\mathclose[0,1\mathclose]$. As long as the returned value is not one, the peer takes the initiative of trying to connect itself to another peer at random, preferring those whose capacity is better than his. During the handshake, any of the intervenients may abort, based on their capacity and the degrees of their neighbors. During this process, the target of the handshake request accepts the requester whenever he does not know enough peers or the requester as more capacity than at least one of his neighbors. To avoid disconnecting poorly connected peers, the target of the handshake request will replace the highest capacity peer whose capacity is smaller than that of the requester. In order to avoid hot-spots or overloading of peers, a peer is only allowed to send requests to a neighbor if that neighbor has explicitly given him one Token; a token represents one request the neighbor is willing to accept; Each peer allocates tokens at the rate at which they can answer requests.

Gia successfully promotes optimal performance and longevity on the systems overlay topology through continuous optimization. Unlike many other unstructured overlays, Gia does not use message flooding to disseminate information, minimizing overhead. There is, however, one fundamental problem with Gia, related to the promotion of fairness; according to their capacity, peers, receive tokens from their neighbors, hence decreasing the usability of the system by low capacity peers. Either way, free riding is not a concept that directly applies to our proposal.\newline

\subsubsection{BlatAnt} \cite{blatant} is a bio-inspired, unstructured overlay, designed for grid computing architectures where resource discovery needs to be efficient; to this end, BlatAnt focuses solely on bounded-diameter optimization in order to minimize network overhead and lookup latency, by using swarm intelligence to support flood-like discovery protocols. Like in Self-Chord, ant agents are used to optimize and maintain a self-structured P2P overlay, conversely, in BlatAnt the ants do not change the location of data items from peer to peer, but instead build and optimize the overlay such that the network uses a minimal number of connections between peers and such that the diameter of the overlay is bounded. The idea is that by doing so, the average number of hops, i.e., average path length, required to obtain a data item from a lookup will be smaller, thus achieving the sought goals. BlatAnt also uses local index caching to improve efficiency further; The authors argue that local caches,  as an alternative to semantic clustering such as the ones used in Self-Chord, are better for self-organized overlays where a stable topology can not be guaranteed. The overlay management can be viewed in three major components.

The first component are the peers themselves. Each peer keeps two structures, a   fixed-size $\alpha$-table, i.e., a partial view of the overlay retaining neighborhood information used to evaluate the redundant connections or the need for new ones, and, a neighborhood-set \textit{N}, containing peer identifiers. Peers contribute to the optimization by rearranging local connections according to the connection rule, which reduces the diameter of the overlay, and disconnection rule, which discards redundant connections.

The second component are the ant agents, which have multiple species; \textit{discovery ants} are randomly spawned by peers and live for a limited time. They wander across the overlay, collecting information about its topology and update the $\alpha$-tables of peers they meet along the way. \textit{Construction ants} act as bootstrappers to the system. When a new peer \textit{i} wants to connect to a peer \textit{j}, he sends one of these ants, if \textit{j} cannot establish the connection because he would violate a degree constraint, he forwards the ant to his lowest-degree neighbor. When a \textit{j'} accepts the ant, he sends it back to \textit{i}, and the procedure complete, i.e., they are now on each others' neighborhood-sets. \textit{Optimization ants} establish connections according to the connection rule. In this case, a peer \textit{i} wanting to connect to peer \textit{j} sends him this ant, and \textit{j} only accepts or refuses the ant. If he accepts, he sends the ant back to \textit{i}. \textit{Unlink ants} remove existing connections between peers because of the disconnection rule applies or because a peer wants to leave the overlay. When this ant arrives at its destination, it removes all sender's information from the $\alpha$-table and the neighborhood-set. \textit{Update Neighbors Ants} are spawned by a peer whenever he establishes or tears-down a connection with some other. These ants visit all of that peer's neighbors and update his information on their respective $\alpha$-table. Finally, \textit{Ping Ants} are periodically exchanged between peers to keep their connections alive in low traffic situations.

The last component are pheromone trails, which evaporate overtime. A trail is a value assigned to a connection between peers, whenever an ant agent, or a query, walks over that connection, they increase the pheromone concentration increases on both ends. Discovery ants will tend to follow paths with less pheromone concentration, thus incentivizing full network coverage during the exploration. When a peer detects that for a given neighbor, its pheromone concentration has wholly evaporated, that neighbor is assumed to have left the network without warning. Eventually, each peer who neighbored the disconnected peer will independently initiate a recovery protocol by sending construction ants to the neighbors of the disconnected peer, reorganizing the overlay, and avoiding network partitioning. Thus pheromone trails provide seamless error resolution.

BlatAnt proved to accomplish its goals, quickly converging to stable overlays with bounded-diameter, even under very dynamic conditions, resulting in faster lookups due to the reduction of flood messages but also due to caching. Note that BlatAnt can leverage random walks such as the ones used in Gia for even less network overhead. BlatAnt also introduces optimizations over caching procedures; we do not review them here, because they are based on complex profile similarities and because caching is out of the scope of our proposal.\newline

\section{Membership Management Frameworks}\label{sec:membership-management}

Gossip-based protocols, also know has epidemic protocols, gained popularity in the nineties for their usability to solve problems like database replication, failure detection, and resource-monitoring. In this section, we present two such protocols used for membership management, i.e., used to keep current partial views of large-scale, dynamic networks, built on unstructured overlays. Just like we mentioned in section \ref{sec:p2p-overlays}, keeping a view of all peers in the system is not a solution. Another fundamental issue of unstructured networks is to avoid partitioning. The following alternatives are both scalable, robust, and decentralized solutions to both these problems. Likewise, peers act autonomously in order to provide self-organization or self-healing capabilities to the overlays in which the protocols are applied.

\subsection{Cyclon} Cyclon\cite{cyclon} is useable even in highly dynamic environments, providing peer degree symmetry, low diameter, low clustering, resilience to churn, and massive node failures. The focus of the authors was to provide a lightweight and simple protocol. It consists of having each peer keeping an ever-changing, partial view, with fixed-size \textit{c}, of the network in a structure called \textit{neighbors}. Periodically, independently and asynchronously, a peer contacts a random neighbor to exchange acquaintances. This process is called \textit{shuffling}. During a shuffle, a peer \textit{i}, first increments the age of all his known neighbors by one. Then, selects the oldest of all his neighbors, \textit{j}, and $l-1$ other random neighbors, forming \textit{s}. Then \textit{i} replaces \textit{j}'s network address by his own and sets his age to zero, in \textit{s}. After that, \textit{i} sends \textit{s} to \textit{j} and waits for \textit{j} to reply with \textit{s'}. Unlike \textit{s}, \textit{s'} is composed of a purely random subset of \textit{j}'s neighbors, without any age modifications. Upon reception of subsets \textit{s} and \textit{s'}, \textit{j} and \textit{i}, respectively, update their \textit{neighbors} by first using any empty slots in their partial views and then by replacing entries which they sent to the other, i.e., if \textit{i} has no more slots, he replaces, in its \textit{neighbors} structure, the entries that he sent in \textit{s} to \textit{j}, with the ones he received in \textit{s'} from \textit{j}. Note that, after \textit{i} initiates a shuffle with \textit{j}, \textit{i} becomes \textit{j}'s neighbor, but \textit{j} is no longer a neighbor to \textit{i}, i.e., the neighboring relation between \textit{i} and \textit{j} is reversed. Unlike many protocols, Cyclon links between peers are not bidirectional.

The age is essential in the shuffling algorithm, for two reasons. First, it limits the lifetime of each peer in \textit{neighbors} structures, which globally bounds the number of existing pointers to them, it also limits the time each peers' addresses are passed around until they are selected as shuffling targets, resulting on a more up-to-date overlay as well as uniform distribution of each peers' addresses over the network.

When a peer leaves the overlay for any reason, the remaining peers may have to remove him from their \textit{neighbors} structure. Timely removal is fundamental for the robustness of the overlay.  Cyclon uses passive detection. Whenever a peer \textit{i} contacts \textit{j} for shuffling and obtains no response it assumes \textit{j} is disconnected and removes it from his \textit{neighbors}. Since the time for \textit{i} to contact \textit{j} is bounded by the age property in \textit{neighbors}, detection is accelerated.

Cyclon simulations show that the average path length and clustering coefficient, for various configurations, converged to values similar to those found in random graphs, i.e., converged to small-diameter topologies with low clustering. Gossip-based protocols often have high clustering, which is undesirable both in terms of robustness and overhead. Another exciting result of Cyclon is that the average path length increased logarithmically, and clustering decreased exponentially, as the number of peers in the network increased, hence demonstrating scalability and increased robustness.

The authors also propose a join method that does not disrupt the randomness of the obtained overlays, while simultaneously making new peers indistinguishable from old ones. Whenever \textit{i} wishes to join the overlay, he contacts one peer \textit{j} already in the overlay who initiates \textit{c} random-walks with time-to-live (TTL) equal to the average path length of the overlay. The peer \textit{k} where the random-walk ends, replaces one of his \textit{neighbors} entries with \textit{i} network address, setting \textit{i} age set to zero and sends the replaced entry to \textit{i}. Note that even if some of the random walks do not complete due to byzantine failures, \textit{i} will remain connected to the overlay and will eventually find new acquaintances.

We conclude by reinforcing that Cyclon provides good self-healing and self-organization capabilities without complex algorithms or apparent downsides other than the one where too few peers in the system may cause it to be under performant due to clustering effects. In a way, it relates to BlatAnt since the proposal passively bounds the diameter of the network.

\subsection{Newscast}
In Newscast \cite{newscast-computing}, all peers proactively exchange, timestamped, information with each other, including not only neighboring information but also app-specific information, allowing for simplified implementation of aggregation algorithms, i.e., finding statistics regarding for example network size. Newscast is responsible for membership-management and a second function, which does not exist in Cyclon, which is information dissemination.

In Newscast, there is a separation of responsibilities. Within each peer, there are two types of entities. Those who run the newscast protocol, are called \textit{corespondents}. Those who run processes of a distributed application are called \textit{agents}. \textit{Correspondents} run the same instance of the newscast protocol. \textit{Agents} are not required to run the same application. Each \textit{agent} has one or more \textit{correspondents}. This separation does not explicitly exist in Cyclon. Each \textit{agent} implements an interface with two functions \textit{getNews()} and \textit{newsUpdate(news[])}. The \textit{correspondent} uses the former to request news from their \textit{agent}, and the latter to deliver app-specific news collected from other peers' \textit{correspondents} in the network. Each \textit{correspondent} as a fixed-size news cache of size \textit{c} and periodically exchange news with each other has followed: request news to the \textit{agent}, timestamp them along with the local time, and IP-address of the \textit{agent}, then storing them in a cache entry. Afterward, it selects another peer's IP address running the same \textit{correspondent} aggregation function, as available in his cache. Both \textit{correspondents} exchange the entirety of their caches and pass the merged cache with $2c+1$ entries to their respective \textit{agents} before discarding the oldest entries, after timestamp normalization, down to \textit{c} and storing the results themselves. The timestamp normalization is not a clock-synchronization process and has some inaccuracy, but as long as transmission times between two peers are not too big, there should be no problem. During the discarding process, the algorithm ensures that there is at most one news item per \textit{agent}.

It is debatable if Newscast has an advantage over Cyclon since news exchange messages already include peer addresses, leading to a transparent resolution of the membership problem. Cyclon, however, has several advantages over Newscast. These include: the resulting overlays of Cyclon have better randomness due to its shuffling-based algorithm, whereas Newscast and other gossip-based management frameworks, seem to have small-world properties according to previous research \cite{eval-gossip-based}. Like in Cyclon, disconnected peers are eventually forgotten due to the timestamp property of the newscasting process. Newly joined peers' cache initialization is based on a regular news exchange with one or more peers who are already members of the overlay. In contrast, Cyclon uses \textit{c} random-walks exchanging exactly one cache entry with possibly different peers, which in turn helps to maintain the randomness properties; finally, while Newscast managed to achieve their goals of providing a robust, scalable, adaptive and lightweight solution for simultaneous membership management and aggregation Cyclon exhibits less network overhead\cite{cyclon}.

\section{Cloud-Assisted P2P Networks}\label{sec:ca-networks}

Many academic and commercial projects have been developed over the years where cloud, grid, and other centralized architectures are partly assisted by P2P networks. In these systems, peers are usually machines belonging to the clients of the service, and they help to decrease the load on the system, hence reducing its cost. This approach is especially prevalent in streaming and online gaming services, for example, Blizzard Activision uses their player base to help distribute installers and patches in a P2P fashion, and Digital Extremes uses, in their free-to-play game Warframe, P2P networks in particular, but not all, group missions where individuals are in instances of the game inaccessible by players outside the group. However, only recently, was the alternative explored, where P2P networks serve the vast majority of the service and are assisted by stable cloud services only to help guarantee some quality of service property of the system under critical situations. We present two of them.

\subsection{Cloudcast}
Cloudcast \cite{cloudcast} is a P2P-assisted cloud architecture that differs from other proposals because it does not offload the provided service to peers only when service availability is not affected. Furthermore, the architecture does not augment P2P systems with elastic computing nodes that perform particular tasks beyond the reach of the P2P network, such has bulk-synchronous content distribution. Cloudcast functionality depends solely on a passive storage service that transparently integrates into a gossip-based P2P system. The reasoning behind not using elastic computing instances is to eliminate fixed renting costs. Like that, the focus is on minimizing the costs associated with storage and bandwidth. The architecture uses two gossip protocols, one for membership management and another for information dissemination. Key results of the proposal include: accesses to the cloud service are bounded regardless of the number of clients in the system; the cloud service only distributes content when few peers are available; the overlay does not suffer from partitioning regardless of the number of peers in the system; the two illustrated use cases have an absurdly low yearly cost when compared to pure cloud approaches, and finally the authors recognize that the vast part of their bill is tied to membership management and not to actual storage costs, thus using strategies that adapt to the size of the network could yield even better savings.

The passive cloud storage service is a key-value store that can be accessed via GET and PUT operations, but it cannot under any circumstance initiate communications with the system peers. Two problems are identified, \textit{membership management} and \textit{information diffusion}. To solve the first problem, Cloudcast uses the Cyclon protocol to do peer-sampling. From other alternatives, Cyclon was chosen not for its particular inexpensiveness, but because the protocol maintains a random overlay topology that works even during high churn periods and even in the presence of catastrophic failures where up to 80\% of the peers fail. Cyclon properties also guarantee that references to the cloud server are not too many keeping usage cost under control. Finally, the cloud server can easily integrate in the protocol, in particular, Cyclon does not require the cloud server to initiate any communications, i.e., its active roles can be carried out by peers interacting with it; Finally, the cloud server can easily integrate into the protocol, in particular, Cyclon does not require the cloud server to initiate any communications, i.e., its active roles can be carried out by peers interacting with it; In this regard, the dummy process carried by the peers occasionally refreshes pointers to the cloud to avoid losing all references to it in the system. To deal with the case where all references cloud are lost due to the dynamic characteristics of the environment, e.g. churn and lost messages, all peers keep a logical timestamp of the last round they heard about the cloud server updating it whenever they successfully contact it or because one of their a \textit{neighbor} heard about it more recently then them, if after \textit{t} rounds peer has not updated this timestamp he will create a new cloud reference with probability $p\textsubscript{recovery}$.

The second problem is solved using two different implementations of gossip algorithms explained in depth in \cite{epidemic_algorithms}. They are a \textit{rumor-mongering, coin and blind} approach which favors fast news spreading and consists in having a peer \textit{i} periodically forwarding updates he learns about to some other peer \textit{j}, then deciding with probability $p\textsubscript{rumor}$ if he should, permanently, stop gossiping the update; And a \textit{anti-entropy, push-pull} approach which guarantees that news are eventually known by all participants of the system and consists in having each peer \textit{i} periodically contacting a random neighbor \textit{j} to exchange update they know about. All updates to the data are recorded in the cloud server through a PUT operation before becoming hot-topics, thus triggering rumor-mongering. Peers never contact the cloud server for rumor-mongering purposes. The anti-entropy approach can be piggybacked to the Cyclon peer-sampling step, thus decreasing message overhead; the authors, however, do not mention this optimization.

\subsection{Cyclon-Cloudcast: Adaptive Replica Management}

\begin{algorithm}[ht]
\caption{MakeDecision(N\textsubscript{e}), replicaOverlay}
\label{alg:makedecision}
\begin{algorithmic}
    \If {$N\textsubscript{e}\geq S$}
        \State $replicaOverlay.Neighbors.Remove(CloudReference)$\
        \If {$N\textsubscript{e} > R$}
            \State $p\textsubscript{yes} \gets \dfrac{N\textsubscript{e} - R}{N\textsubscript{e}}$
            \State $this.LeaveOverlay(replicaOverlay, p\textsubscript{yes})$
        \State $replicaOverlay.Protocol \gets cyclon$
        \EndIf
    \ElsIf {$C < N\textsubscript{e} < S$}
        \State $N\textsubscript{a} \gets [\dfrac{S(1 + k)}{N\textsubscript{e}} - 1]$
        \State $newNeighbors \gets GetNewNeighbors(this.UnderlyingOverlay, N\textsubscript{a})$
        \State $newNeighbors.Send(this.RandomView(replicaOverlay))$
        \State $replicaOverlay.Neighbors.Add(newNeighbors)$
        \State $replicaOverlay.Protocol \gets cloudcast$
    \ElsIf{$Ne \leq C$}
        \State $N\textsubscript{a} \gets [\dfrac{S(1 + k)}{N\textsubscript{e}} - 1]$
        \State $newNeighbors \gets GetNewNeighbors(this.UnderlyingOverlay, N\textsubscript{a})$
        \State $newNeighbors.Send(this.RandomView(replicaOverlay) \cup CloudReference)$
        \State $replicaOverlay.Neighbors.Add(newNeighbors \cup CloudReference)$
        \State $replicaOverlay.Protocol \gets cloudcast$
    \EndIf
\end{algorithmic}
\end{algorithm}

Following the suggestions of Cloudcast authors, H. Kavalionak \textit{et al.}, developed a decentralized algorithm \cite{marriage_of_convinience} that aims to do replica management in cloud-based, peer-assisted applications. Because the proposed work has no running title, we shall refer to it as ARM. The goal of the ARM is to maintain an appropriate replication level, despite churn, such that, at all times, at least one replica is available and ensuring that replicas survive failures. Furthermore, the proposal acknowledges that replicas should be synchronized, i.e., the system provides at least some consistency guarantees. In simple terms, this is perhaps the main difference between the proposed work and Cloudcast, which stores all updates in an always-on passive cloud server, helping with information diffusion and overlay connectivity and overall reliability of the system in extreme conditions. Cloudcast monetary cost reductions are primarily associated with limiting access to the cloud server. ARM effectively reduces those costs even further by performing timely switches between Cloudcast and regular Cyclon protocols, depending on the size of the network. To estimate the network size without centralization or broadcasting algorithms that would ultimately lead to either bottlenecks or overhead, ARM leverages a gossip-based aggregation method \cite{gossip-based_aggregation} that allows each peer to infer the approximate size of the network autonomously, at the end of each epoch. Estimation accuracy is dependent only on the size of the epoch. Likewise, the execution cost depends on the length of the epochs and the network churn rate, but not on the number of peers in the system, which means that ARM keeps the scale-free properties of the unstructured overlays that used in Cloudcast. The peers randomly organize themselves according to the Cyclon peer-sampling protocol, but depending on the results of the aggregation, the passive cloud-server used in Cloudcast is removed or kept in the overlay. In ARM, each data object to be replicated as its \textit{replica-overlay} on top of the regular unstructured overlay, which facilitates synchronization and cloud-usage decisions. The ARM algorithm is designed to consider four replica-overlay states, separated by three size thresholds: \textit{Critical (C)}, \textit{Sufficient (S)} and \textit{Redundant (S)}. \textit{C} depends on the chosen redundancy method, i.e., file-level \textit{vs.} block-level replication and is the sum of the minimum number of replicas required for data recovery plus the number of peers that will leave during the cloud backup replication phase. \textit{S} depends on the churn-rate and is the sum of \textit{C} plus the number of peers expected between two successive recovery phases. Finally, \textit{R} is a system parameter dependent also on the chosen redundancy method and is application requirements specific.  The aggregation method mentioned earlier is the foundation \textit{monitoring phase}, which consists of a fixed number of gossip and idle rounds, allowing a peer to infer the \textit{expected size (N\textsubscript{e})} of the replica-overlay and decide if he should add or remove the cloud or peers to the replica-overlay as well as the protocol he uses, based on Algorithm \ref{alg:makedecision}. After each monitoring phase, each peer calculates how long it should wait before executing the \textit{monitoring phase} again. That time depends mostly on \textit{N\textsubscript{e}}, e.g., if the state is below the critical threshold, the monitoring phase will initiate earlier than if it is above the redundant threshold, allowing the system to respond to overlay changes promptly and to keep the overlay size in a range that minimizes overhead but ensures that data does not become unavailable. The authors do not specify how much savings the system was capable of achieving using ARM; however, we can assume it is a substantial value; In Fig. \ref{fig:arm_results}, the plot on top displays the number of peers in the underlying network as a function of time, and the bottom plot shows the in-degree to the cloud server as a function of time, comparing Cloudcast and ARM (labeled as "our protocol").

\begin{figure}[!ht]
\centering
\includegraphics[width=\textwidth]{images/armresults.PNG}
\caption{Cloud-Server in-degree, Cloudcast \textit{vs.} ARM \cite{marriage_of_convinience}}
\label{fig:arm_results}
\end{figure}

\newpage\section{Data Redundancy and File Survivability}\label{sec:dataredundancy}
In distributed storage systems, not losing files is of the utmost importance. Particularly, since peer availability in P2P networks is ever-changing and not guaranteed, the availability and even durability of a file may be endangered. A \textbf{\textit{durable file}} is one that, once put into the system, is not lost due to failures, whereas availability means that a user can access it promptly. For clarity, an available file is durable, but the other way is not necessarily correct. Distributed systems as a whole, usually ensure durability by employing file-level or block-level \textbf{\textit{replication}}; alternatively, some systems adopt \textbf{\textit{erasure coding}} techniques (\textit{coding} for short) or a mixture of both approaches. Replication has the advantage of having a more straightforward and less resource-intensive recovery model, on the other hand, coding has a better \textit{expansion factor}, i.e., can provide the same or even increased durability at a lower storage overhead. Because our proposal focuses on the survivability of files in P2P networks, using swarm guidance, where resources may be limited, we care only about coding in the survey that follows, briefly mentioning some of the raised concerns and inherent disadvantages of this technique.

Two parameters specify some of the most widely used coding algorithms, the number of data symbols \textit{k} to be encoded, and the number of coded symbols \textit{n} to be produced, both assumed to be in a finite field $GF(2^w)$ and these are referred as \textit{(n, k) erasure-codes}. The fundamental idea, is that a file of size $S$ is divided into $k$ equally sized fragments and encode them into $n$ encoded fragments, allowing the original file to recovered from any set of encoded-fragment with count $k$. Systems that use these and other more sophisticated coding are evaluated under some of the following metrics, the \textbf{\textit{repair-bandwidth}}, i.e., the number of bits communicated in the network during normal and repair operations, the \textbf{\textit{disk I/O}} which is the number of bits read from disk during each repair, the \textbf{\textit{repair locality}} which is the number of peers that would be involved in the repair process; Since coding algorithms are slow processes CPU cycles, XOR operation counts and cache misses are metrics often researched as well \cite{fast_coding}, when the main interest is tied with load-balancing and cost savings. We take particular interest in the first metric because, in general, coding may lead to unreasonable bursts in bandwidth consumption when availability requirements, in very dynamic systems, are high \cite{coding-problems}, even for today's standards. Whenever a peer fails, all files he held need to be quickly recovered by some peer who needs to gather enough fragments from possibly many different peers. Such burst in consumption may lead to delays or even failure of the repair process due to physical link capacity between peers, who are involved in the repair process, or even throughput constraints enforced by each peer. When it comes to redundancy in a distributed storage systems, one fundamental question is deciding when to replicate or recover data; this is often called the replica-control problem.

There are two main approaches, reactive and proactive. Reactive approaches are the least complex, and consist of recovering data whenever a failure is detected; they suffer the most from bandwidth bursts, and mitigations to this problem include use of predictive models and thresholds to avoid unnecessary recoveries \cite{lifetime-reactive, efficient-replica-mng-reactive}. Proactive approaches seek to smooth bandwidth consumption by injecting new redundancy data into the network overtime at a fixed-rate \cite{proactive-rep, reliability-without-availability}.

The most widely known coding algorithms belong to the \textit{Maximum Distance Separable} (MDS) class, this class is likely to offer the best reliability-redundancy trade-off. Regardless of the chosen approach, research \cite{network-coding-for-dss} suggests that for coding to be competitive in distributed storage systems, one should seek to minimize these costs, while maximizing redundancy to only the amount necessary to ensure a specific service guarantee. They identify \textit{Minimum Bandwidth Regenerating} (MBS), and all codes in between these two ends of the spectrum as \textit{regenerating codes}, the differences between codes on the spectrum depend on the values such has \textit{n}, \textit{k} and on the implementation of the algorithms themselves. Shokrollahi \textit{et al.} introduces raptor codes \cite{raptor-codes}, which are a linear-time coding and decoding extension of LT codes. LT codes belong to the class of rateless codes, i.e., a potentially infinite sequence of encoded fragments can be generated from a given data source. Furthermore, the original data is recoverable from any subset of the encoded fragments of size equal to or slightly larger than original data size; These rateless code implementations are of particular importance because they are scalable with regards to the machine and network resources,  and fault-tolerant concerning dynamic and heterogeneous network environments. Finally, Zhou \textit{et al.} \cite{fast_coding} perform a study on state of the art techniques that improve coding efficiency through bitmatrix optimization, vectorization, and scheduling and prove that when used in conjunction much better levels of performance can be delivered, e.g., 552.27\% when compared to simple MDS implementations, in particular, the Reed-Solomon algorithm.

\newpage\section{Probabilistic Swarm Guidance}\label{sec:psg}
Markov chains describe a sequence of possible events in which the probability of each event depends only on the previous event states, often used to model real-world processes, such as queues of customers arriving at an airport and population growths. Markov processes are the basis for stochastic simulation methods known as Markov Chain Monte Carlo (MCMC), which allow one to perform random sampling over vast state spaces, i.e., computing expectations concerning complex, high dimensional probability distributions. The idea behind MCMC is creating a Markov Chain, which will, over time, tend to an equilibrium that is close to desired density distribution. In robotics and control areas Markov Chains and MCMC are used as the basis foundation for a process known as probabilistic swarm guidance, which gifts autonomous robots with the capability of generating their trajectories independently of each other, in decentralized fashion and without ahead-of-time position allocation, so that sets of robots, known as swarm, converges to the desired distribution shape. Swarm guidance has the advantage over other methods for this problem because robots do not necessarily need to communicate with each other to perform their tasks, even if they could gain from it; their overall distribution can be adapted dynamically to the environment, and any damage to their desired formation is eventually self-repaired. Demonstration of these properties can be found in recent research by B. Açikmeçe \textit{et al.} \cite{psg-mca, psg-caa}, which inspired us to apply swarm guidance principles to P2P networks and file survivability. Boyd et al. \cite{fast_mixing_mc} argue that the most popular MCMC methods, Metropolis-Hastings and Maximum-Degree Chain, that allow the creation of transition matrices, that can then be followed independently by autonomous robots in order to practice swarm guidance behavior are not optimal with respect to mixing rates, i.e.,  these algorithms may not result in matrices that converge in small amount of steps towards the desired density distribution. They propose that the problem of obtaining such matrices can be formulated as a convex optimization problem and prove that, for most cases, the resulting matrices have higher mixing rates. It is reasonable to assume that, concerning our problem in P2P networks, this might open the possibility of imposing additional constraints on the creation of the transition matrix, e.g., bounding the probability of a peer sending a file to some other peer in his hive to increase bandwidth balancing.

%%%%%%%%%%%%%%%%%%%%
% END RELATED WORK %
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN SOLUTION PROPOSAL %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Solution Proposal}\label{sec:proposal}
In this section, we introduce a prototype architecture that targets the use case that inspired to explore the theme of this thesis. We explain it by decomposition.

\subsection{Use Case}
Fundamentally, the idea is that a company, Hives, with dedicated web servers called Hiveminds, provides a platform for registered users to sell storage belonging to their personal computers directly to other registered users. Clients pay a monthly fee directly to Hives based on how much storage space they need. Hiveminds then put together a set of sellers, which constitute a Hive, that serves adequate storage space to the clients. Sellers are selected according to the reputation they earn over time based on their reliability, with a bias towards more reputable sellers. The collected fee is distributed proportionately among Hive members according to their overall space contribution and up time, calculated weekly, for example. The company's revenue comes from keeping a small percentage of the monthly fees. If the client requires continuous availability, rather than durability, he may pay a premium, in which case, inspired on the related work mentioned in section \ref{sec:ca-networks}, cloud storage may be used to help the Hive in meeting the required service guarantees. Hive revenue shares are reduced if their reliability degrades too much.

\subsection{Key notes and Assumptions}
\begin{itemize}
    \item Session handling and payment models are out of the scope of this proposal.
    \item Reputation is static, i.e., they are simulation parameters.\footnote{See goals in Section \ref{sec:intro}}
    \item Message integrity and confidentiality are not at risk.
    \item Sellers are not malicious, e.g., do not perform DoS on other sellers.
    \item Sellers may fail without warning, and some absences may be transient.
    \item Cloud service is abstract, i.e., ranges from local to cloud storage servers.
    \item The storage space purpose is backups, thus write operations are rare.\footnote{Write operations possibly cause old versions to be overwritten}
    \item The storage space is not shared, i.e., there are no concurrent writes.
\end{itemize}

\subsection{Hiveminds - Master Servers}
The Hivemind servers are the backbone of the system; they have to handle registration, login, subscription and payment procedures. When a client wishes to subscribe to a monthly plan, he contacts one Hivemind server at random by accessing his desktop application, and the Hivemind then sets a \textit{usedSpace} variable indicating how much data the Client uploaded into the system to zero. This variable is updated according to the Client's future requests. In Hives File System (Hives), users' files are uniquely identified and have no hierarchy. Hiveminds keep a mapping between the \textit{fileId} and the Hive\footnote{Set of peers S responsible for holding one or more files from one or more clients} that is responsible for storing them. Hive membership is dynamic, and a mapping between the \textit{HiveId} and the Hive members (Workers) is also kept. Note that it is not relevant to know which fragments each Worker in the Hive has; this is important because it diminishes the number of messages sent to Hiveminds and also reduces space overhead required to keep track of file locations. When a Hivemind receives a file upload request from a Client, it finds or creates a suitable Hive to host the data and calculates, if needed, the transition matrix used for PSG within it, before returning all relevant data to the Client, including Workers' addresses. When the Hivemind receives a delete request, it drops all mapping data regarding the respective \textit{fileId}.

\subsection{Clients}
After registration, logging in, and subscription procedures, Clients, can perform four main procedures, which we name and implement using HTTP semantics. In all cases, the Client first retrieves the Hive Workers' addresses. In a GET request, the Client downloads enough fragments from the Hive Workers and reconstructs the file himself. In a DELETE request, the Client first asks the Hivemind to stop tracking efforts, and once he receives an acknowledge, he broadcasts in a best-effort manner a DELETE message to any online worker within that file's Hive. In a POST operation, the Client first obtains permission to upload from the Hivemind; after that, he divides his file into parts, applies an erasure coding algorithm over each them, and uploads the encoded fragments to the workers at random. PUT updates existing files and operate similarly to POST, but whenever possible, try to send the new version to the Hive that contains the previous \textit{versionId}.

\subsection{Workers - Sellers}
Apart from adequately dealing with clients' requests, Workers organize themselves in a P2P fashion within each Hive. One Worker can be in zero or more Hives, and within each Hive, they continuously redistribute encoded file part fragments according to the transition matrix that is calculated by the Hivemind and delivered to them by a Client. In the long run, we expect that a Hive achieves the desired fragment distribution that fairly distributes load while ensuring optimal durability. Constantly exchanging fragments is expected to cause a bandwidth overhead but has the advantage that Clients can upload their fragments to any online peer or peers in the Hive instead of one single entity that may go offline during the upload. Workers will then redistribute the fragment using simple routing rules. This approach does not sacrifice fast lookup operations due to the structured nature of the system using a Hivemind that behaves somewhat like a BitTorrent tracker. The never-ending fragments exchange between peers also facilitates the timely detection of non-transient faults in the Hive, which helps to ensure file durability. Workers piggyback aggregate information to the fragments, including but not limited to, how many parts of a file they have, allowing us to implement a hybrid between a reactive and a proactive system concerning replica control, in hopes of minimizing bandwidth burst on top of already high bandwidth requirements. Fragment exchange may eventually be replaced with ping-like messages to minimize bandwidth cost further, hence the importance of having a fast mixing rate transition matrix. When a Worker suspects that some other Worker on a Hive is faulty for too long, he first contacts a dedicated server, called Keeper, who decides the new Hive membership. The suspecting Worker then individually decides based on previous aggregate information if he should inject new fragments regarding the file into the network if he believes durability is at risk. Whenever a Worker who has been temporarily offline reconnects, he first contacts the master to know what \textit{fileId} is he is no longer responsible for maintaining, allowing him to free space. They also perform this procedure whenever they do not receive a fragment or a ping regarding a \textit{fileId} for a sufficiently long period.

\subsection{Keepers}
Keepers' sole responsibilities are: performing limited hop random walks on the network to gather data that might be used on a Reputation System and to decide new Hive membership on behalf of the Workers who detect faults and possibly on behalf of the Hiveminds.


%%%%%%%%%%%%%%%%%%%%%%%%%
% END SOLUTION PROPOSAL %
%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN EVALUATION METHODOLOGY %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation Methodology}\label{sec:methodology}
We will first evaluate the performance of different transition matrix generating algorithms by comparing their theoretical fast mixing property against the number of discrete time steps the matrices take to converge on a simple, cycle-based, locally ran, custom-built simulator. We will select one of these algorithms based on how fast convergence occurs, computing time vs. space complexity, and the possibility of allowing each peer to update their transition matrix autonomously when faced with transient neighbor faults. After this initial step, we will implement a reactive replica-control policy using erasure-coding at each peer and test the durability of files on our custom-built simulator, where we have perfect failure detectors. We are concerned with discovering a good trade-off between storage overhead and durability by using different (n, k) values without caring for any networking aspects. Finally, we will measure bandwidth consumption as well as the robustness of our solution under more realistic simulations by either implement our protocol on well-known P2P Simulation test-bed such has PeerfactSim.KOM\cite{peerfact}, or if time permits it, deploy proof of concept applications on the university's computer cluster for distributed testing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% END EVALUATION METHODOLOGY %
%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN WORK SCHEDULE %
%%%%%%%%%%%%%%%%%%%%%%%
\newpage\section{Work Schedule}\label{sec:workschedule}
%%%%%%%%%%%%%%%%%%%%%
% END WORK SCHEDULE %
%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%
% BEGIN CONCLUSIONS %
%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%
% END CONCLUSIONS %
%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
% BEGIN REFERENCES %
%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{splncs04}
\bibliography{bibliography}
%%%%%%%%%%%%%%%%%%
% END REFERENCES %
%%%%%%%%%%%%%%%%%%

\end{document}
%%%%%%%%%%%%%%%%
% END DOCUMENT %
%%%%%%%%%%%%%%%%
